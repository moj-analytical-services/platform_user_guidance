[
["index.html", "Analytical Platform User Guidance About this guidance Feedback", " Analytical Platform User Guidance MoJ Analytical Platform Team 2019-07-30 About this guidance This guidance provides information on how to use the Analytical Platform. Feedback If you have any feedback on this guidance, please get in touch by email or on Slack. You can also report any issues on GitHub. "],
["introduction.html", "Part 1 Introduction 1.1 What is the Analytical Platform? 1.2 Get started 1.3 Access the Analytical Platform 1.4 Work with analytical tools 1.5 Configure Git and GitHub 1.6 Contact us 1.7 Slack", " Part 1 Introduction 1.1 What is the Analytical Platform? The Analytical Platform is a cloud–based system that provides a range of services, tools and resources to analysts across MoJ. The Analytical Platform: provides access to the latest versions of open–source analytical software, such as RStudio and JupyterLab, allowing analysts to work in the way that suits them best allows analysts to freely install packages from CRAN and PyPI, enabling them to utilise advanced analytical tools and techniques, including text mining, predictive analytics and data visualisation uses Amazon S3 and Athena to provide a single location for all of our data, including a growing set of curated data, and GitHub to provide a single location for all of our code – this enables analysts to collaborate more effectively, share knowledge and produce high–quality reproducible analysis allows analysts to build and deploy interactive apps and web pages that can be used to communicate analysis and insight to decision–makers gives analysts the tools to develop reproducible analytical pipelines (RAPs) to automate time–consuming and repetitive tasks, allowing them to focus on interpreting the results is built in a cloud–based ecosystem that is easy to access remotely from all MoJ IT systems and provides analysts with access to powerful computational resources that can be scaled to meet demand – this allows analysts to quickly and easily work with big data and perform complex analytical tasks is secure, resilient and has high availability – this means analysts can depend on it to store their work and data, and to complete time–sensitive and business–critical projects 1.2 Get started To get started on the Analytical Platform, you should complete the following steps: Read our acceptable use policy and coding standards. Sign up for a GitHub account. Verify your email address for GitHub. Configure two-factor authentication for GitHub. Request an account from the Analytical Platform team. Accept your invitation to the MoJ Analytical Services GitHub organisation. Configure two-factor authentication for the Analytical Platform. 1.2.1 Read our acceptable use policy and coding standards Use of the Analytical Platform is subject to our acceptable use policy. You should ensure that you have read and understood this before getting started on the Analytical Platform. You should also follow our coding standards when working on the Analytical Platform. These set out principles that you should follow when writing and reviewing code. 1.2.2 Sign up for a GitHub account To sign up for a GitHub account, go to GitHub’s join page and follow the instructions. When signing up, we recommend that you use your work email address. When instructed to choose your subscription, you should select the free plan. It is good practice to choose a username that does not contain upper-case characters. You should use a secure password following best practice guidelines. We recommend that you use a password manager, such as LastPass, to generate and store strong passwords. 1.2.3 Verify your email address for GitHub Verifying your email address for GitHub ensures that your account is secure and gives you access to all of GitHub’s features. Veryifying your email address is also required to sign in to the Analytical Platform. During sign up, GitHub will send you an email with a link to verify your email address. If you do not verify your email address at this stage, you can do it later by following the instructions here. 1.2.4 Configure two-factor authentication for GitHub To get access to the Analytical Platform, you must first configure two-factor authentication (2FA) for GitHub using a mobile app (on your personal or work phone) or via text message. To configure 2FA for GitHub, follow the instructions here. We recommend that you configure 2FA using a mobile app. In particular, we recommend that you use Authy for the following reasons: You can sync 2FA tokens across multiple devices, including mobiles, tablets and computers. You can create encrypted recovery backups in case you lose your device. You can use Touch ID, PIN codes and passwords to protect access to your 2FA tokens. You can still access secure websites when you don’t have phone signal. 1.2.5 Request an account from the Analytical Platform team To request an account for the Analytical Platform, you should send an email with your GitHub username to the Analytical Platform team. The team will then invite you to join the MoJ Analytical Services GitHub organisation. 1.2.6 Accept your invitation to the MoJ Analytical Services GitHub organisation When you are invited to join the MoJ Analytical Services GitHub organisation, you will receive an email with a link to accept the invitation. You can also accept your invitation by signing in to GitHub and visiting the organisation page. 1.2.7 Configure two-factor authentication for the Analytical Platform When you sign in to the Analytical Platform for the first time, you will be prompted to configure additional two-factor authentication (2FA) for the Analytical Platform itself. To sign in, go to the Analytical Platform control panel. You must configure 2FA for the Analytical Platform using a mobile app (on your personal or work phone). As described in Section 1.2.4, we recommend that you use Authy. 1.3 Access the Analytical Platform The main entry point to the Analytical Platform is the control panel. From here, you can access RStudio and JupyterLab and can manage your warehouse data sources. You may also need to access other services and tools on the Analytical Platform: Airflow AWS console Concourse Grafana Kibana 1.4 Work with analytical tools 1.4.1 Deploy analytical tools Before using RStudio, JupyterLab and Airflow, you must first deploy them. To deploy RStudio, JupyterLab and Airflow on the Analytical Platform, you should complete the following steps: Go the Analytical Platform control panel. Select the Analytical tools tab. Select the Deploy buttons next to RStudio, JupyterLab and Airflow. It may take a few minutes for the tools to deploy. 1.4.2 Open analytical tools To open RStudio, JupyterLab or Airflow: Go the Analytical Platform control panel. Select the Analytical tools tab. Select the Open button to the right of the tool’s name. 1.4.3 Restart analytical tools If your RStudio, JupyterLab or Airflow is not working as expected, it may help to restart it. To restart RStudio, JupyterLab or Airflow: Go the Analytical Platform control panel. Select the Analytical tools tab. Select the Restart button to the right of the tool’s name. 1.4.4 Unidle analytical tools If your RStudio, JupyterLab or Airflow instance is inactive for an extended period of time (for example, overnight) it will be idled. Go the Analytical Platform control panel. Select the Analytical tools tab. Select the Unidle button to the right of the tool’s name. Unidling usually only takes a few seconds, however, it can take up to several minutes. 1.5 Configure Git and GitHub To configure Git and GitHub for the Analytical Platform, you must complete the following steps: Create an SSH key. Add the SSH key to GitHub. Configure your username and email in Git on the Analytical Platform. 1.5.1 Create an SSH key You can create an SSH key in RStudio or JupyterLab. We recommend that you use RStudio. 1.5.1.1 RStudio To create an SSH key in RStudio, follow the steps below: Open RStudio from the Analytical Platform control panel. In the menu bar, select Tools then Global Options… In the options window, select Git/SVN in the navigation menu. Select Create RSA key… Select Create. Select Close when the information window appears. Select View public key. Copy the SSH key to the clipboard by pressing Ctrl+C on Windows or ⌘C on Mac. 1.5.1.2 JupyterLab To create an SSH key in JupyterLab, follow the steps below: Open JupyerLab from the Analytical Platform control panel. Select the + icon in the file browser to open a new Launcher tab. Select Terminal from the ‘Other’ section. Create an SSH key by running: ssh-keygen -t rsa -b 4096 -C &quot;your_email@example.com&quot; Here, you should substitute the email address you used to sign up to GitHub. When prompted to enter a file in which to save the key, press Enter to accept the default location. When prompted to enter a passphrase, press Enter to not set a passphrase. View the SSH key by running: cat /home/jovyan/.ssh/id_rsa.pub Select the SSH key and copy it to the clipboard by pressing Ctrl+C on windows or ⌘C on Mac. 1.5.2 Add the SSH key to GitHub To add the SSH key to GitHub, you should follow the guidance here. 1.5.3 Configure you username and email in Git on the Analytical Platform To configure your username and email in Git on the Analytical Platform using RStudio or JupyterLab, follow the steps below: Open a new terminal: In RStudio, select Tools in the menu bar and then Shell… In JupyterLap, select the + icon in the file browser and then select Terminal from the Other section in the new Launcher tab. Configure your username by running: git config --global user.name &#39;Your Name&#39; Here, you should substitute your GitHub username. Configure your email address by runnung: git config --global user.email &#39;your_email@example.com&#39; Here, you should substitute the email address you used to sign up to GitHub. Further guidance on using Git and GitHub with the Analytical Platform can be found in Section 3. 1.6 Contact us To get support, you can contact the Analytical Platform team on the #analytical_platform Slack channel or by email. You can get more information on Slack in Section 1.7. For more information on support, including incident response, please see our key support information. 1.7 Slack 1.7.1 What is Slack? Slack is a collaboration tool that helps teams work more effectively together. You can use Slack in several ways: to get technical support for the Analytical Platform and analytical tools, such as R, Python and Git to submit admin requests relating to apps and data sources on the Analytical Platform to share knowledge, expertise and best practice to communicate quickly with other Analytical Platform users as an alternative to email 1.7.2 Access Slack You can access our Slack workspace here. To create an account, you will need an email address ending in @justice.gsi.gov.uk, @digital.justice.gov.uk, @cjs.gsi.gov.uk, @noms.gsi.gov.uk, @legalaid.gsi.gov.uk, @justice.gov.uk or @judiciary.uk. It is not mandatory to join or use Slack (although we highly recommend it) so you shouldn’t use it to make any important announcements and should ensure that you are not excluding anyone from the discussion. You may have to access Slack differently depending on the IT system you use: if you use DOM1, you can access Slack from a browser or using the mobile app on your personal or work phone if you use an MoJ Digital and Technology MacBook, you can access Slack from a browser or using the desktop app. You can also access Slack using the mobile app on your personal or work phone if you use Quantum, you will not currently be able to access Slack from your computer, however, you can use the mobile app on your personal or work phone 1.7.3 Channels Conversations in Slack are organised into channels, which each have a specific topic. Channels can be public (all users can join) or private (users can only join by invitation) and can be created based on teams, projects, locations, tools and techniques among others. There are several public channels that are widely used and may be useful to join: the #git, #r and #python channels can be used to get support from other users with any technical queries or questions – the #intro_r channel is aimed specifically at new users of R the #analytical_platform channel is used for general discussion of the Analytical Platform – it is also monitored by the Analytical Platform team, who can help out with any technical queries the #ap_admin_request channel is used to request new apps and app data sources as well as access to existing apps and data sources the #data_engineers channel is used for general discussion of data engineering and for getting in touch with the data engineering team as well as to submit Airflow DAG pull requests for review the #data_science channel is used for general discussion of data science tools and techniques the #general channel is used for any discussions that don’t fit anywhere else the #10sc channel can be used to get in touch with other people working at 10SC and to ask any questions about the building There are lots of other channels you can join or you can set up a new one if you think something is missing. You may also wish to set up private channels to discuss specific projects and can direct message individual users. 1.7.4 Further information You can find out more about Slack and how to use it in the Slack Help Centre. "],
["amazon-s3.html", "Part 2 Amazon S3 2.1 What is Amazon S3? 2.2 Working with Amazon S3 buckets 2.3 Upload files to Amazon S3 2.4 Download or read files from Amazon S3", " Part 2 Amazon S3 2.1 What is Amazon S3? Amazon S3 is a web-based cloud storage platform. It is one of the primary file storage locations on the Analytical Platform, alongside individual users’ home directories. You should use your home directory to store working copies of code and analytical outputs. Where possible, you should store all data and final analytical outputs in Amazon S3, and final code in GitHub to facilitate collaboration. Data stored in Amazon S3 can be seamlessly integrated with other AWS services such as Amazon Athena and Amazon Glue. 2.2 Working with Amazon S3 buckets 2.2.1 Types of buckets Amazon S3 buckets are separated into two categories on the Analytical Platform: warehouse data sources app data sources Warehouse data sources are suitable for storing files in all cases, except where the files need to be accessed by a webapp. In this case, files should be stored in an app data source. 2.2.2 Create a new bucket Standard users can currently only create new warehouse data sources in the Analytical Platform control panel. You cannot create new buckets directly in the Amazon S3 console. To create a new warehouse data source: Go to the Analytical Platform control panel. Select the Warehouse data tab. Select Create a new warehouse data source. Enter a name for the warehouse data source – this must be prefixed with ‘-alpha’. Select Create. When you create a new warehouse data source, only you will initially have access. As an admin of the data source, you will be able to add and remove other users from the data access group as required. Further information on managing data access groups can be found in Section 2.2.5. 2.2.3 Access levels Every bucket has three access levels: Read only Read/write Admin – this provides read/write access and allows the user to add and remove other users from the bucket’s data access group 2.2.4 Request access to a bucket To gain access to a bucket (warehouse data source or app data source), you must be added to the relevant data access group. If you know an admin of the bucket you require access to, you should ask them to add you to the data access group. If you do not know any of the admins of the bucket you require access to, you can ask the Analytical Platform team to add you to the data access group on the #ap_admin_request Slack channel or by email. When requesting access to a bucket, you should specify the name of the bucket and the level of access you require. You should only request access to data that you have a genuine business need to access and should only request the lowest level of access required for you to complete your work. You may be required to demonstrate the business need for you to access a bucket if requested by a bucket admin or an information asset owner (IAO). 2.2.5 Manage access to a bucket Bucket admins can currently only manage access to warehouse data sources in the Analytical Platform control panel. You cannot manage access to buckets directly in the Amazon S3 console. To manage access to a warehouse data source: Go to the Analytical Platform control panel. Select the Warehouse data tab. Select the name of the warehouse data source you want to manage. To add a new user to the data access group: Type the user’s GitHub username into the input field labelled Grant access to this data to other users. Select the user from the drop-down list. Select Grant access. To edit the access level of a user: Select Edit access level next to the name of the user. Select the checkbox next to the access level you wish to grant the user. Select Save. To remove a user from the data access group: Select Edit access level next to the name of the user. Select Revoke access. 2.3 Upload files to Amazon S3 You can upload files to Amazon S3 from your local computer or from RStudio or JupyterLab. When uploading files to Amazon S3, you should ensure that you follow all necessary information governance procedures. In particular, you must complete a data movement form when moving any data onto the Analytical Platform. 2.3.1 Amazon S3 Console You can use the Amazon S3 Console to upload files from your local computer (for example, personal or shared storage on DOM1 or Quantum) only. To upload files using the Amazon S3 Console: Log in to the AWS Management Console using your Analytical Platform account. Select Services from the menu bar. Select S3 from the drop down menu. Select the bucket and folder you want to upload files to. Select Upload. Select Add files or drag and drop the files you want to upload. Select Upload – you do not need to complete steps 2, 3 or 4. You can also directly navigate to a bucket in the AWS S3 Console by selecting Open on AWS in the Analytical Platform control panel. 2.3.2 RStudio You can upload files in RStudio on the Analytical Platform to Amazon S3 using the s3tools package. s3tools should be preinstalled for all users of the Analytical Platform. If you find that s3tools is not installed, you can install it by running the following code: install.packages(&#39;remotes&#39;) library(remotes) remotes::install_github(&#39;moj-analytical-services/s3tools&#39;) s3tools contains three functions for uploading files to Amazon S3: write_file_to_s3 write_df_to_csv_in_s3 write_df_to_table_in_s3 You can find out more about how to use these functions on GitHub or by using the help operator in RStudio (for example, ?s3tools::write_file_to_s3). 2.3.3 JupyterLab You can upload files in JupyterLab on the Analytical Platform to Amazon S3 using the boto3 package. You can install boto3 by running the following code in a terminal: pip install boto3 To upload a file to Amazon S3, use the following code: import boto3 s3 = boto3.resource(&#39;s3&#39;) s3.object(&#39;bucket_name&#39;, &#39;key&#39;).put(Body=object) If you receive an ImportError, try restarting your kernel, so that Python recognises your boto3 installation. Here, you should substitute 'bucket_name' with the name of the bucket, 'key' with the path of the object in Amazon S3 and object with the object you want to upload. You can find more information in the package documentation. 2.4 Download or read files from Amazon S3 2.4.1 Amazon S3 Console You can use the Amazon S3 Console to download files to your local computer (for example, personal or shared storage on DOM1 or Quantum) only. To download a file using the Amazon S3 Console: Log in to the AWS Management Console using your Analytical Platform account. Select Services from the menu bar. Select S3 from the drop down menu. Select the file you want to download. Select Download or Download as as appropriate. You can also directly navigate to a bucket in the AWS S3 Console by selecting Open on AWS in the Analytical Platform control panel. 2.4.2 RStudio You can download or read files in RStudio on the Analytical Platform from Amazon S3 using the s3tools or s3browser packages. 2.4.2.1 s3tools s3tools should be preinstalled for all users of the Analytical Platform. If you find that s3tools is not installed, you can install it by running the following code: install.packages(&#39;remotes&#39;) library(remotes) remotes::install_github(&#39;moj-analytical-services/s3tools&#39;) s3tools contains four functions for downloading or reading files from Amazon S3: download_file_from_s3 s3_path_to_df s3_path_to_full_df s3_path_to_preview_df You can find out more about how to use these functions on GitHub or by using the help operator in RStudio (for example, ?s3tools::download_file_from_s3). 2.4.2.2 s3browser s3browser provides a user interface within R that allows you to browse files you have access to in Amazon S3. You can install s3browser by running the following code: install.packages(&#39;remotes&#39;) library(remotes) remotes::install_github(&#39;moj-analytical-services/s3browser&#39;) To open the browser, run: s3browser::file_explorer_s3() You can find out more about how to use s3browser on GitHub. 2.4.3 JupyterLab 2.4.3.1 pandas You can use any of the pandas read functions (for example, read_csv or read_json) to download data directly from Amazon S3. This requires that you have installed the s3fs package. To install the s3fs package, run the following code in a terminal: pip install s3fs As an example, to read a CSV, you should run the following code: import pandas as pd pd.read_csv(&#39;s3://bucket_name/key&#39;) Here, you should substitute bucket_name with the name of the bucket and key with the path of the object in Amazon S3. 2.4.3.2 boto3 You can also download or read objects using the boto3 package. You can install boto3 by running the following code in a terminal: pip install boto3 To download a file from Amazon S3, you should use the following code: import boto3 s3 = boto3.resource(&#39;s3&#39;) s3.Object(&#39;bucket_name&#39;, &#39;key&#39;).download_file(&#39;local_path&#39;) If you receive an ImportError, try restarting your kernel, so that Python recognises your boto3 installation. Here, you should substitute 'bucket_name' with the name of the bucket, 'key' with the path of the object in Amazon S3 and local_path with the local path where you would like to save the downloaded file. You can find more information in the package documentation. "],
["github.html", "Part 3 Using Github with the platform 3.1 Creating your project repo on GitHub 3.2 R Studio 3.3 Jupyter 3.4 Command-line 3.5 Working on a branch 3.6 Git training resources 3.7 Safety barriers 3.8 Private R packages on GitHub: PAT authentication 3.9 Other tips and tricks [Work in progress!]", " Part 3 Using Github with the platform Before you can use Github with R Studio or Jupyter, you need to connect them together by creating an ‘ssh key’. Full guidance is here Github enables you to collaborate with colleagues on code and share you work with them. It puts your code in a centralised, searchable place. It enables easier and more robust approaches to quality assurance, and it enables you to version control your work. More information about the benefits of Github can be found here. If you are new to Git and Github, it is worth clarifying the difference between Git and Github. Git is the software that looks after the version control of code, whereas Github is the website on which you publish and share your version controlled code. In practice this means you use Git to track versions of your code, and then submit those changes to Github. This guide provides a step-by-step guide of how to create a GitHub project repo, followed by how to sync with it in R Studio and Jupyter. You can find more in-depth Git training resources here Note: If any of the animated gifs below do not display correctly, try a different web browser e.g. Microsoft Edge, which is installed on your DOM1 machine. 3.1 Creating your project repo on GitHub 3.1.1 Step 1 - Create a new project repo in the moj-analytical-services Github page A GitHub ‘repo’ (short for ‘repository’) is conceptually similar to setting up a project folder on the DOM1 shared drive to save your work, and share it with others. The files in this Github repo represent the definitive version of the project. Everyone who works on the project makes contributions to this definitive version from their personal versions. Note that if you want to contribute to an existing project, you can skip this step. In your web browser go to github.com and make sure you’re signed in. Once signed in, go to the MoJ Analytical Services homepage at https://github.com/moj-analytical-services/ Then follow the steps in this gif to create a new repository. create repo Notes: Leave your repository ‘private’ for now - the default setting. In the next step you will add access to colleagues and possibly make it ‘public’ (on the internet). Make sure the owner is set to ‘moj-analytical-services’. This is the default setting, so long as you have clicked on ‘New’ from the https://github.com/moj-analytical-services homepage. 3.1.2 Step 2: Navigate to your new Repository on GitHub to decide who can see your code Try to be as open as possible about who can view your code. Go to the Settings section of your repository (top right of the repository’s homepage) and then click on Collaborators &amp; Teams on the left hand side panel. From there you can then decide on one of the four options below. They start with the most private all the way to completely public code: PRIVATE: Leave the default setting of your repository so it’s only visible to you as the creator. YOUR TEAM: Can the code be shared within your team? If so, add your team to the repository. ALL PLATFORM USERS: Can the code be shared with all Analytical Platform users? If so, add the ‘everyone’ team to the repository. PUBLIC: Can the code be public? If so, make it a public repository. To do this, click on the ‘Options’ section of the Settings, then scroll down to the ‘Danger Zone’ area that has a ‘Make public’ button. We find that for most of our work, there’s no reason not to add the ‘+everyone’ team of all Analytical Platform users with read access to the code. This is possible as sensitive datasets are not stored in Github. By making code more open (either internally or publicly), users can start to get much more value of out the extremely powerful code search in GitHub. Warning: Repos should contain no passwords/secrets and no data (apart from small reference tables) - this is particulary important for public repos, but applies to private ones too. And remember that GitHub shows the full history of files and changes in your repo, so removing these things requires special effort. For more info, see: Choosing public or private repos Notes: you can add one or more teams to a repository, each with different permissions. For example, your team could have write privileges, but the ‘everyone’ team could be read only. 3.2 R Studio Here’s how you can sync with your new GitHub repo in R Studio. 3.2.1 Step 1: Navigate to your platform R Studio and make a copy of the Github project in your R Studio In this step, we create a copy of the definitive GitHub project in your personal R Studio workspace. This means you have a version of the project which you can work on and change. Follow the steps in this gif: clone repo Notes: When you copy the link to the repo from Github, ensure you use the ssh link, which start git@github.com as opposed to the https one, which start https://github.com/ 3.2.2 Step 2: Edit your files, track them using Git, and sync (‘push’) changes to Github Edit your files as usual using R Studio. Once you’re happy with your changes, Git enables you to create a ‘commit’. Each git commit creates a snapshot of your personal files on the Platform. You can can always undo changes to your work by reverting back to any of the snapshots. This ‘snapshotting’ ability is why git is a ‘verson control’ system. In the following gif, we demonstrate changing a single file, staging the changes, and committing them. In reality, each commit would typically include changes to a number of different files, rather than the single file shown in the gif. stage changes Notes: ‘committing’ does not sync your changes with github.com. It just creates a snapshot of your personal files in your platform disk. Git will only become aware of changes you’ve made after you’ve saved the file as shown in the gif. Unsaved changes are signified when the filename in the code editor tab is red with an asterisk. 3.2.3 Step 3: Sync (‘push’) your work with github.com In R Studio, click the ‘Push’ button (the green up arrow). This will send any change you have committed to the definitive version of the project on Github. You can then navigate to the project on Github in your web browser and you should see the changes. push to github Notes: After pushing, make sure you refresh the GitHub page in your web browser to see changes. That’s it! If you’re working on a personal project, and are not collaborating with others, those three basic steps will allow you to apply version control to your work with Github 3.3 Jupyter There is not the same integration. Use the command-line - see below. 3.4 Command-line Once you are comfortable using the Terminal (in either R Studio or Jupyter) you can do steps 1 to 3 above using the following git commands: Clone the repo (using the SSH link from Github) to copy it to a new folder in your working directory: git clone &lt;SSH link&gt; Change directory to the new folder: cd &lt;repo_name&gt; Select the files that you want to commit: git add &lt;filename1&gt; &lt;filename2&gt; ((This will ‘add’ them to git’s ‘index’ / ‘staging’ area) ‘Commit’ the files you have added: git commit. After calling this command, you need to provide a commit message: in R Studio it provides a popup; in Jupyter it’ll start an editor where you write the message, before saving and exiting it. ‘Push’ your commits to GitHub: git push origin &lt;branch_name&gt;. Most likely your branch name will be master which is the default. So your code would be git push origin master. 3.5 Working on a branch One of the most useful aspects of git is ‘branching’. This involves a few extra steps, but it enables some really important benefits: Allows you to separate out work in progress from completed work. This means there is always a single ‘latest’ definitive working version of the code, that everyone agrees is the ‘master copy’. Enables you and collaborators to work on the same project and files concurrently, resolving conflicts if you edit the same parts of the same files. Enables you to coordinate work on several new features or bugs at once, keeping track of how the code has changed and why, and whether it’s been quality assured. Creates intutitive, tagged ‘undo points’ which allow you to revert back to previous version of the project e.g. we may wish to revert to the exact code that was tagged ‘model run 2015Q1’. We therefore highly recommend using branches. (Up until now, we’ve been working on a single branch called ‘master’.) 3.5.1 Step 1 (optional): Create an Issue in github that describes the piece of work you’re about to do (the purpose of the branch) Github ‘issues’ are a central place to maintain a ‘to do’ list for a project, and to discuss them with your team. ‘Issues’ can be bug fixes (such as ‘fix divide by zero errors in output tables’), or features (e.g. ‘add a percentage change column to output table’), or anything else you want. By using issues, you can keep track of who is working on what. If you use issues, you automatically preserve a record of why changes were made to code. So you can see when a line of code was last changed, and which issue it related to, and who wrote it. 3.5.2 Step 2: Create a new branch in R Studio and tell Github about its existence Create a branch with a name of your choosing. The branch is essentially a label for the segment of work you’re doing. If you’re working on an issue, it often makes sense to name the branch after the issue. To create a branch, you need to enter the following two commands into the shell: git checkout -b my_branch_name. Substitute my_branch_name for a name of your choosing. This command simultaneously creates the branch and switches to it, so you are immediately working on it. git push -u origin my_branch_name. This tells github.com about the existence of the new branch. 3.5.3 Step 3: Make some changes to address the Github issue, and push (sync) them with Github Make changes to the code, commit them, and push them to Github. 3.5.4 Step 4: View changes on Github and create pull request You can now view the changes in Github. Github recognises that you’ve synced some code on a branch, and asks you whether you want to merge these changes onto the main ‘master’ branch. You merge the changes using something called a ‘pull request’. A ‘pull request’ is a set of suggested changes to your project. You can merge these changes in yourself, or you can ask another collaborator to review the changes. One way of using this process is for quality assurance. For instance, a team may agree that each pull request must be reviewed by a second team member before it is merged. The code on the main ‘master’ branch is then considered to be quality assured at all times. Pull requests also allow you and others working on the project to leave comments and feedback about the code. You can also leave comments that reference issues on the issue log (by writing # followed by the issue number). For example you might comment saying “This pull request now fixes issue #102 and completes task #103”. 3.5.5 Step 5: Sync the changes you made on github.com with your local platform When you merged the pull request, you made changes to your files on Github. Your personal version of the project in your R Studio hasn’t changed, and is unaware of these changes. The final step is therefore to switch back to the ‘master’ branch in R Studio, and ‘Pull’ the code. ‘Pulling’ makes R Studio check for changes on Github, and update your local files to incorporate any changes. 3.6 Git training resources If you are new to git and you want to learn more, we recommend that you complete the basic tutorial available here. The slides from from the ASD git training are available here (dom1 access only) Using Github with R Introductory interactive tutorial. Quickstart guide and cheatsheet here and in pdf format here. More in depth materials: Learn Git branching Git from the inside out 3.7 Safety barriers The platform has configured simple “safety barriers” to reduce risk of accidentally exposing sensitive data on GitHub. For example it stops you committing a CSV file, because in most circumstances you should not put data into GitHub - it should be kept in an S3 bucket where it can be shared with authorized people. These rules can be overridden if that makes more sense. What How it’s configured Reasoning How to override Data files (.csv, .xls etc) &amp; zip files ~/.gitignore You should not put data into GitHub - it should be kept in an S3 bucket where it can be shared with authorized people. When you add the file: git add -f &lt;filename&gt; Zip files ~/.gitignore It’s better to unpack these files and commit the raw source. You can’t keep track of diffs of individual files if you keep them bundled up. There might be a data file lurking in the zip, which isn’t checked if it is bundled like this. Note: git has its own built in compression methods. When you add the file: git add -f &lt;filename&gt; Large files (&gt;5 Mb) ~/.git-templates/hooks/pre-commit Likely to be data When you commit: git commit --no-verify Pushing to non-official GitHub organizations ~/.git-templates/hooks/pre-push It would be outside MoJ control - not normally allowed. When you push: git push -f &lt;remote&gt; &lt;branch&gt; 3.8 Private R packages on GitHub: PAT authentication 3.8.1 Private and public repositories GitHub repositories can be public or private. In either case, repos should contain no passwords/secrets and no data (apart from small reference tables) - this is particulary important for public repos, but applies to private ones too. And remember that GitHub shows the full history of files and changes in your repo, so removing these things requires special effort. A public repo is visible to the world. Again, it is particulary important these contain no passwords/secrets or data. A Private repo is internal to the moj-analytical-services GitHub organisation and not visible to the outside world. The repo’s Owners and Admins can control which people / teams can see the repo on GitHub by going to Settings &gt; Collaborators &amp; Teams and adding teams to read/write/admin access groups. To maximise collaboration within the organisation but still keep code that shouldn’t be in the public domain (e.g. unpublished commentary on a .Rmd file) hidden from anyone outside the organisation, you can add the group everyone here. This also makes your code searchable. 3.8.1.1 Choosing public or private repos As an organization we aspire to use public repos by default. There are a host of benefits of coding in the open. With research and analysis it builds trust and transparency with the public, and reproducible methods allows others to test and build on your work. However, it requires more discipline to avoid mistakes like slipping secrets and sensitive information, so tends to require more experienced developers and care over any political sensitivities related to the topics your analysis covers: open-source coding is continuous and worldwide publishing. As a result, sometime Private repos can be necessary, for example when it reveals a sensitive policy change that is not yet announced. If in doubt, discuss with your manager and/or the AP team. 3.8.2 Private R packages for reproducible analysis When a repo (e.g. an R package) is private you need to use a Personal Access Token (PAT) to access it from R. This token acts much like a password to your personal GitHub account. If you don’t use the token then you get: Error: HTTP error 404. Not Found. (GitHub doesn’t even acknowledge the existence of the repo, to avoid speculative searching for private repositories.) 3.8.3 Generating a PAT You can get a PAT from your GitHub user profile. A user can generate a number of tokens that grant various different levels of permission. The PAT must be kept secret and protected like a password. It must not be shared with other users. Only generate your PAT to have the minimum permissions needed for the job. To generate your PAT for reading private repos in R: Navigate to Settings &gt; Developer Settings &gt; Personal Access Tokens Select Generate new token and select the scope of access you want to grant using this token. IMPORTANT: Only select the first group: ‘repo’. Granting other rights can be dangerous if your PAT falls into the wrong hands, allowing someone else to irrevocably delete your repos, read and change your user profile settings, or even access billing information. If you need to to use other rights, consult with the AP team for advice on security arrangements to protect the PAT. Copy your PAT to the clipboard. It is now ready to paste directly into the place it will be used, which is likely to be your R or Python environment - see the next section. Note: Although GitHub won’t show this PAT again, DON’T save it elsewhere. If you need a PAT in the future (eg you wiped your .rstudio folder) simply create a new PAT, and revoke the previous one. Storing secrets unnecessarily, particularly in plain text, is a security hazard. 3.8.4 Using a PAT to authenticate in R/RStudio You should store your PAT in an R profile file in your home directory (on AP). This file gets run when you start R, putting the PAT into the system environment variable GITHUB_PAT. This is where common R packages (e.g. remotes and devtools) will look for it. Set this up by running this in your R console (you only need to run it once; do not save this line in any file in any repo) and substitute your own PAT Token in place of the example PAT ax451...8838b1 below: writeLines(&#39;Sys.setenv(&quot;GITHUB_PAT&quot; = &quot;ax451...8838b1&quot;)&#39;, con = &quot;~/.Rprofile&quot;) Warning: This line should not be put in any repo, and this .Rprofile file should not be added to any repo. Now restart your R Session (in the menu ‘Session’ -&gt; ‘Restart R’). You can check it worked by typing in your R console to see the token: Sys.getenv(&quot;GITHUB_PAT&quot;) Now your code can include remotes::install_github to install an R package from which is in a private repo using the token. For example: r remotes::install_github(&quot;moj-analytical-services/[private-package]@v1.6&quot;) Putting the secret in ~/.Rprofile - a file outside your repo directories - avoids the serious error of putting the token (a secret) in your repo. Security reminder Never put a secret or password in your code. Even when the repo is private. See MOJ policy: https://ministryofjustice.github.io/security-guidance/standards/secrets-management/#application--infrastructure-secrets 3.9 Other tips and tricks [Work in progress!] 3.9.1 Search the code in MoJ Analytical Services to see who else has used a package. An example of a code search is here. Further guidance is here 3.9.2 Hyperlink to a specific line of code in your project See here for how to do this. 3.9.3 View who made changes to code, when and why using Git blame. See here. An example of this is here. 3.9.4 Make your project available to people on different teams 3.9.5 Assign a reviewer to a pull request, and leave comments. 3.9.6 View how files have changed on the platform and on "],
["deploying-an-r-shiny-app.html", "Part 4 Deploying an R Shiny app 4.1 Basic deployment 4.2 Advanced 4.3 Troubleshooting", " Part 4 Deploying an R Shiny app 4.1 Basic deployment 4.1.1 Summary To create and deploy a Shiny app, you should complete the following steps: Use the app template. Create a new webapp. Clone the repository. Develop the app. Manage dependencies. Set access permissions. Create a release in GitHub. Deploy in Concourse. Add users to the app. Access the app. 4.1.2 Use the app template To create a new repository based on the Shiny app template: Go to the rshiny-template repository. Select Use this template. Fill in the form: Owner: moj-analytical-services Name: The name of your app, for example, my-app Privacy: Private Select Create repository from template. This copies the entire contents of the app template to a new repository. 4.1.3 Create a new webapp Standard users are not able to create new webapps or webapp data sources themselves. To create a new webapp or webapp data source, ask the Analytical Platform team on the #ap_admin_request Slack channel or by email, if you are a Quantum user. You should provide the URL of the app’s GitHub repository as well as any existing webapp data sources it should be connected to. 4.1.4 Clone the repository To clone the repository: Navigate to the app’s repository on GitHub. Select Clone or download. Ensure that the dialogue says ‘Clone with SSH’. If the dialogue says ‘Clone with HTTPS’ select Use SSH. Copy the SSH URL. This should start with git@. In RStudio, select File &gt; New project… &gt; Version control &gt; Git. Paste the SSH URL in the Repository URL field. Select Create Project. 4.1.5 Develop the app Develop the app in RStudio. Your app can take one of several forms: A directory containing server.R, plus, either ui.R or a www directory that contains the file index.html. A directory containing app.R. An .R file containing an R Shiny application, ending with an expression that produces an R Shiny app object. A list with ui and server components. an R Shiny app object created by shinyApp. By default, the template contains server.R and ui.R files, however, you may wish to take a different approach depending on your requirements. For example, using app.R, it is possible to deploy R Shiny apps from within a package, as here. 4.1.6 Manage dependencies Most apps will have dependencies on various third-party packages (e.g., dplyr). These packages change through time and may not always be backwards compatible. To avoid compatibility issues and ensure reproducible outputs, it is necessary to use a package management system, such as packrat or Conda. If using packrat, ensure that it is enabled for your project in RStudio. To enable packrat, select Tools &gt; Project Options… &gt; Packrat &gt; Use packrat with this project. When packrat is enabled, run packrat::snapshot() to generate a list of packages used in the project, their sources and their current versions. You may also wish to run packrat::clean() to remove unused packages from the list. The list is stored in a file called packrat/packrat.lock. You must ensure that you have committed this file to GitHub before deploying your app. 4.1.7 Set access permissions You can set some access permissions for your app in the deploy.json file that is included with the app template. This file is used by Concourse to detect apps that are ready to build and deploy. The allowed_ip_ranges parameter controls where your app can be accessed from. It can take any combination of the following values [&quot;DOM1&quot;, &quot;QUANTUM&quot;, &quot;102PF Wifi&quot;] or [&quot;Any&quot;]. The disable_authentication parameter controls whether sign-in (using a link or one-time passcode sent to an authorised email address) is required for users to access the app. It can take the values true or false. In general, this should be set to false. When disable_authentication is set to true, users do not need to go through a sign-in process but can still only access an app using a system specified in allowed_ip_ranges. This is a relatively weak security measure, as discussed here. As such, if you wish to disable authentication, you should first discuss this with the Analytical Platform team. 4.1.8 Create a release in GitHub When you’re ready to share your app, you should create a release in GitHub. When you create a release, this is detected by Concourse, which will automatically begin the build/deploy process for your app. Each time you create a new release, Concourse will create a new build. To create a release in GitHub: Navigate to the app’s repository. Select release &gt; Draft a new release. Choose a tag version for the release – GitHub provides tagging suggestions at the right of the screen that we advise you to follow. Choose a title for the release. Describe the contents of the release. Select Publish release. 4.1.9 Deploy in Concourse Once you have created a release in GitHub, Concourse should automatically start to deploy your app within a few minutes. If your app does not deploy automatically, you should first check that the pipeline is not paused. If the app still does not deploy automatically, you can manually trigger a build by pressing the + icon in the top right corner of Concourse. For more information about using Concourse, see Section 7. 4.1.10 Add users to the app If disable_authentication is set to false in the deploy.json file, access to the app will be controlled by email address. You can ask the Analytical Platform team to add or remove users to the access list on the #ap_admin_request Slack channel or by email, if you are a Quantum user. 4.1.11 Access the app Your deployed app can be accessed at repository-name.apps.alpha.mojanalytics.xyz, where repository-name is the name of the relevant GitHub repository. If the repository name contains underscores, these will be converted to dashes in the app URL. For example, an app with a repository called repository_name would have the URL repository-name.apps.alpha.mojanalytics.xyz. When accessing an app, you can choose whether to sign in using an email link (default) or a one-time passcode. To sign in with a one-time passcode, add /login?method=code to the end of the app’s URL e.g. https://kpi-s3-proxy.apps.alpha.mojanalytics.xyz/login?method=code. (This requires the app to have been deployed since the auth-proxy release on 30/1/19.) 4.2 Advanced 4.2.1 Editing the Dockerfile A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. In most cases, you will not need to change the Dockerfile when deploying your app. If your app uses packages that have additional system dependencies, you will need to add these in the Dockerfile. If you are unsure how to do this, contact the Analytical Platform team. A Dockerfile reference can be found here. 4.2.2 Getting the current user of the app A Shiny app can find out who is using it. This can be useful to log an audit trail of significant events. Specifically it can determine the email address that the user logged into the app with. (Obviously this is sensitive data and needs data protection concerns looking after - e.g. proportionate, transparent, consent, secure etc). See the example: https://github.com/moj-analytical-services/shiny-headers-demo 4.3 Troubleshooting 4.3.1 Common errors Sometimes, an R Shiny app can deploy successfully but result in the following error: An error has occurred The application failed to start The application exited during initialization This is generic error that means there is an error in your R code or there are missing packages. To try to fix this you should: explicitly reference all third-party packages using the double colon operator (i.e. use shiny::hr() as opposed to hr()) ensure that you have called packrat::snapshot() and committed packrat.lock to GitHub, if using packrat In general, it is also good practice to: minimise the number of packages you use in your project test your app early and often test using a cloned copy of the app’s repository to avoid issues arising as a result of uncomitted local changes 4.3.2 App sign-in Some anti-virus software and spam filters pre-click links in emails, meaning that app sign-in links do work. In this case, you should sign in using a one-time passcode, as described in Section 4.1.11. 4.3.3 Packrat If you are having issues with packrat.lock, follow the steps below: Delete the entire packrat directory. Comment out all code in the project. Enable packrat using packrat::init(). Capture all package dependencies using packrat::snapshot(). Uncomment all code in the project and install package dependencies one by one. Rerun packrat::snapshot(). Redeploy the app. 4.3.4 Kibana All logs from deployed apps can be viewed in Kibana. To view all app logs: Select Discover from the left sidebar. Select Open from the menu bar. Select Application logs (alpha) from the saved searches. To view the logs for a specific app: Select Add a filter. Select app_name as the field. Select is as the operator. Insert the app name followed by ‘-webapp’ as the value. Select Save. Log messages are displayed in the message column. By default, Kibana only shows logs for the last 15 minutes. If no logs are available for that time range, you will receive the warning ‘No results match your search criteria’. To change the time range, select the clock icon in the menu bar. There are several presets or you can define a custom time range. Kibana also has experimental autocomplete and simple syntax tools that you can use to build custom searches. To enable these features, select Options_ from within the search bar, then toggle Turn on query features. 4.3.5 Deploying locally If you have a MacBook, you can use Docker locally to test and troubleshoot your R Shiny app. You can download Docker Desktop for Mac here. To build and run your R Shiny app locally, follow the steps below: Clone your app’s repository to a new folder on your MacBook – this guarantees that the app will be built using the same code as on the Analytical Platform. Open a terminal session and navigate to the directory containing the Dockerfile using the cd command. Build the Docker image by running: docker build . -t &lt;image tag&gt; where image tag is a tag you want to give the image. Run a Docker container created from the Docker image by running: docker run -p 80:80 &lt;image tag&gt; Go to 127.0.0.1:80 to view the app. If the app does not work, follow the steps below to troubleshoot it: Start a bash session in a Docker container created from the Docker image by running: docker run -it -p 80:80 &lt;image tag&gt; bash Install the nano text editor by running: apt-get update apt-get install nano Open shiny-server.conf in the nano text editor by running: nano /etc/shiny-server/shiny-server.conf Add the following lines at the beginning of shiny-server.conf: access_log /var/log/shiny-server/access.log tiny; preserve_logs true; Write the changes by pressing Ctrl+O. Exit the nano text editor by pressing Ctrl+X. Increase the verbosity of logging and start the Shiny server by running: export SHINY_LOG_LEVEL=TRACE /bin/shiny-server.sh Open a new terminal session. Start a new bash session in the Docker container by running: docker exec -it &lt;CONTAINER ID&gt; bash You can find the CONTAINER ID by running docker ps. View the logs by running: cat /var/log/shiny-server/access.log For further details, see the Shiny server documentation. "],
["deploying-a-static-web-app.html", "Part 5 Deploying a Static Web App 5.1 Step-by-step guide to depolying an static web app 5.2 Accessing the app 5.3 Advanced deployment", " Part 5 Deploying a Static Web App The following steps to deploy a static HTML web site are as follows: Copy the template project within Github to a new repository, with a name of your choice. Work on your static website - the exposed content will be in the www/ directory and www/index.html will be the landing page. When you’re ready to share it, access the services control panel, find your app, and click ‘Build now’. This will prepare your site for deployment. Set the desired access permissions in deploy.json (i.e. whether it should be available for DOM1, Quantum, or external). When you’re ready to share it, in GitHub create a ‘release’ and it will deploy in a few minutes. Ask the Platform team to add and remove users from your app in the #ap_admin_request Slack channel. Step-by-step instructions are below. 5.1 Step-by-step guide to depolying an static web app 5.1.1 Use the webapp template To create a new repository based on the webapp template: Go to the webapp-template repository. Select Use this template. Fill in the form: Owner: moj-analytical-services Name: The name of your app, for example, my-app Privacy: Private Select Create repository from template. This copies the entire contents of the app template to a new repository. 5.1.2 In your chosen development enviroment, clone the git repository You can find the clone link on the Github repository. To download a copy to start editing on your local machine, you need to ‘clone’ the repositry. If you’re using a shell: git clone git@github.com:moj-analytical-services/YOUR-REPO-NAME.git 5.1.2.1 Further notes if you’re having trouble finding your new repo’s url If you navigate to your new repository’s home page (which will have a url in the form https://github.com/moj-analytical-services/your_name_goes_here), you can use the following buttons to access this url (make sure you click the ‘ssh’ button): 5.1.3 Work on your web app Work on your web app using your chosen development enviroment. As you work, commit your changes to Github using your chosen Github workflow. 5.1.4 Scan organisation and deploy Include a deploy.json file so that Concourse will automatically build and deploy your app, so that it is running and customers can access it. For more about this see: Build and deploy guidance 5.1.5 Grant secure access to the app To grant access to someone, in the Control Panel’s Wepapps tag find your App and click “Manage App”. In the ‘App customers’ section you can let people view your app by putting one or more email addresses in the text box and clicking “Add customer”. You can also let anyone access it by setting &quot;disable_authentication&quot;: true in the deploy.json and redeploying it - see above. Note that users can only access the app from a computer on a specified corporate network. These are defined in the deploy.json under the parameter allowed_ip_ranges - see below. 5.1.5.1 deploy.json parameter notes allowed_ip_ranges e.g. [&quot;DOM1&quot;, &quot;QUANTUM&quot;, &quot;102PF Wifi&quot;] or [&quot;Any&quot;] disable_authentication To let anyone access the app, set this to true, otherwise false restricts it to people authorized for the app in the Control Panel If you deployed with authentication enabled users are granted access to the app using a list of email addresses separated with a space, comma or semicolon. Changes to deploy.json only take effect when committed to GitHub, a Release is created and the deploy is successful (see Scan organisation and deploy) 5.2 Accessing the app Depending on the disable_authentication setting, the website will either be available directly or authenticated via email. The URL for the app will be the respository-name followed by apps.alpha.mojanalytics.xyz. So for the example project above “static-web-deploy”, the deployment URL will be https://static-web-deploy.apps.alpha.mojanalytics.xyz. Note that characters that are not compatible with website URLs are converted. So, repositories with underscores in their name (e.g. repository_name.apps...) will be converted to dashes for the URL (e.g. repository_name.apps...). 5.3 Advanced deployment This section contains guidance for advanced users on app deployment. 5.3.1 Can I change my build? Yes - if you know Docker, you are welcome to change the Dockerfile. "],
["data-pipelines.html", "Part 6 Data pipelines 6.1 Summary 6.2 Concepts 6.3 Set up a pipeline 6.4 Test pipeline in your own Airflow sandbox", " Part 6 Data pipelines 6.1 Summary You can deploy your data processing code to the cloud. Airflow is a tool on the Analytical Platform that is a managed place for your “data pipeline” to run. This can be useful, for example, to: run a long job overnight run it on a regular schedule (e.g. every night) when multiple outputs require the same intermediate result, it can be calculated once for them all people other than yourself can see the run and its output (compared to if you run it in your R Studio or Jupyter) you keep a history of all the pipeline runs, showing what you ran, the logs, what tasks failed and how long it took 6.2 Concepts A data pipeline is referred to in Airflow as a “DAG”. It is made up of “tasks” which are arranged into a Directed Acyclic Graph. A DAG is a Directed Acyclic Graph. This could be simple: Task 1 -&gt; Task 2 -&gt; Task 3 (meaning run Task 1 then Task 2 then Task 3). Or a task can be dependent on multiple previous tasks, and when its complete it can trigger multiple tasks. The “Acyclic” bit just means you can’t have loops. Each task is a GitHub repository, containing code files that will be run (e.g. R or python) plus bits that define the environment it runs in - a Dockerfile and an AWS IAM policy. You define the DAG to run on a regular schedule, and/or you can run it by clicking the “Play” button on the Airflow web interface 6.3 Set up a pipeline 6.3.1 Task repository Each task should be a git repository: It should be under the https://github.com/moj-analytical-services organization The name of the repo is recommended to start with airflow- The repo should include: Dockerfile - contains commands to set-up a Docker image that does your data processing. Here’s an example python one: # Begin with a standard base image (from https://hub.docker.com/_/python/ ) FROM python:3.7 # Update the system libraries # (The python base image is based on debian) RUN apt-get update # Install new system libraries RUN apt-get install -y --no-install-recommends \\ python-numpy # Set the default place in the image where files will be copied WORKDIR /usr/src/app # Copy files from the repo into the image COPY requirements.txt ./ COPY . . # Install python dependencies RUN pip install --no-cache-dir -r requirements.txt # Run the data processing # This is the equivalent of typing: python ./my-script.py CMD [ &quot;python&quot;, &quot;./my-script.py&quot; ] Tip: You can put several related scripts in one repo, and choose which one gets run using an environment variable e.g. https://github.com/moj-analytical-services/airflow-magistrates-data-engineering/blob/88986cf7dbcca9b7ebc21bc2d1286464615739d7/Dockerfile#L19 Tip: Some python packages, such as ‘numpy’ or ‘lxml’, come with C extensions which can cause bother. If installed via pip (requirements.txt) those C extensions are compiled, which can be slow and liable to failure. The Dockerfile example above shows how to avoid these problems with numpy (which is needed by pandas) - it is installed instead using a debian package: python-numpy. deploy.json - This is for Concourse to build the repo into a Docker image For example: { &quot;mojanalytics-deploy&quot;: &quot;v1.0.0&quot;, &quot;type&quot;: &quot;airflow_dag&quot;, &quot;role_name&quot; : &quot;airflow_enforcement_data_processing&quot; } The version number does nothing. The role_name needs to be: unique to the Analytical Platform prefixed airflow_, otherwise you’ll get a build error like this: An error occurred (AccessDenied) when calling the GetRole operation: User: arn:aws:iam::593291632749:user/dev-concourse-role-putter is not authorized to perform: iam:GetRole on resource: role airflow-prison-population-hub iam_policy.json - This tells Concourse to create an AWS role that gives your code permission to access AWS resource, such as data in S3 buckets. e.g. See: https://github.com/moj-analytical-services/airflow-enforcement-data-engineering/blob/master/iam_policy.json for listing, reading and writing to S3 buckets. 6.4 Test pipeline in your own Airflow sandbox You can deploy your own sandboxed version of Airflow to test DAGs and docker images on. 6.4.1 Start Airflow sandbox Click the Deploy button next to airflow-sqlite in the Analytical tools section of the control panel. This should take a few minutes to deploy. Then click the Open button to open your Airflow sandbox in a new window. This will create an airflow directory in your home directory on the Analytical Platform. In this airflow directory there are three more directories: db, dags and logs. The dags directory is where you will store your test DAG files. 6.4.2 Create a test DAG Using JupyterLab on the Analytical Platform, create a Python file in the airflow/dags directory in your home directory on the Analytical Platform (e.g., test_dag.py). Airflow will automatically scan this directory for DAG files every three minutes. You must use you own IAM role and set the namespace in the dag to your own Kubernetes namespace. See above for creating a Docker image to run the DAG if you are using KubernetesPodOperator. Concourse will automatically build images from pull requests for Airflow repositories in GitHub that will be tagged with the name of the pull request. For example, if your pull request branch is named test-dev then your image will be named my_repositiry:test-dev. Example DAG file contents: {#ex-code} from datetime import datetime from airflow.contrib.operators.kubernetes_pod_operator import KubernetesPodOperator from airflow.utils.log.logging_mixin import LoggingMixin from airflow.models import DAG log = LoggingMixin().log args = {&quot;owner&quot;: &quot;airflow&quot;, &quot;start_date&quot;: datetime(2019, 3, 18)} dag = DAG( dag_id=&quot;example_kubernetes_operator_assume_role&quot;, default_args=args, schedule_interval=None, ) k = KubernetesPodOperator( namespace=&quot;user-###YOUR_GITHUB_USERNAME###&quot;, image=&quot;governmentpaas/awscli:latest&quot;, cmds=[&quot;aws&quot;, &quot;--debug&quot;], arguments=[&quot;s3api&quot;, &quot;list-objects&quot;, &quot;--bucket&quot;, &quot;###YOUR_TEST_BUCKET###&quot;], env_vars={ &quot;AWS_METADATA_SERVICE_TIMEOUT&quot;: &quot;60&quot;, &quot;AWS_METADATA_SERVICE_NUM_ATTEMPTS&quot;: &quot;5&quot;, }, labels={&quot;foo&quot;: &quot;bar&quot;}, name=&quot;airflow-test-pod&quot;, in_cluster=True, task_id=&quot;task&quot;, get_logs=True, dag=dag, is_delete_operator_pod=True, annotations={&quot;iam.amazonaws.com/role&quot;: &quot;alpha_user_###YOUR_GITHUB_USERNAME###&quot;}, ) You will have to set the DAG namespace to your namespace and the AWS role to your role, see ###YOUR_GITHUB_USERNAME### in the above example code. You will also need to update the image with your DAG image from the AWS Elastic Container Registry and add any environment variables, etc., as you would with a normal DAG. If you update the image it is likely you will not need to add the arguments cmds and arguments to the KubernetesPodOperator so those lines should be removed. They are the command and arguments that are run on the pod at startup. 6.4.3 Important considerations This sandbox Airflow is meant for testing. You should not use it to run regular tasks and should not rely on it for running production pipelines. It is likely that the sandbox will be idled when it is not being used. Because DAGs are run as your own alpha_user_..., they will have the same access permissions. DAGs are not run using an iam_policy.json, even if one exists in the task GitHub repository. This means that you may need to either obtain access to any required buckets or copy data into a bucket that you already have access to. You should still be very careful with what you do as even with your role you may still have access to do a lot of damage. 6.4.4 Test your image - locally on a Macbook If you have a Macbook you can use it to test that your Docker image will build, which is much quicker to debug than waiting for Concourse every time you make a change. This can be useful for checking your Dockerfile syntax is right and the dependencies install ok. However when you run the Docker image, it won’t be able to access your project data (from S3), because your machine doesn’t have your AP account’s AWS credentials. This is right because you shouldn’t have sensitive data on your Macbook in general. So the Docker image will fail when it tries to access AWS data, but you’ve still usefully checked the build works and the install of dependencies. You need to have installed Docker for Mac. When it is running you can see its whale icon in the task bar. It consumes 2GB RAM, so you may well not want it to run everytime your machine starts up - change this under “Preferences…” then uncheck “Start Docker when you log in”. To test your Docker image: Clone your repo to your Macbook. e.g. in a terminal: git clone git@github.com:moj-analytical-services/airflow-occupeye-scraper.git This authenticates GitHub using SSH, so if this is the first time on your machine, you’ll need to create an SSH key and add it to your GitHub account first. Build the repository (from the directory with the Dockefile in it): cd airflow-occupeye-scraper docker build . -t my-pipeline:0.1 (If you get Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? you need to start ‘Docker’ application from your Mac’s Launchpad) This goes through each line (“step”) of the Dockerfile and builds (or executes) each in turn (apart from the final CMD). The first build can take a while, but the result of each step is saved to disk, so if you change a step and do another build, it only needs to restart from the step that is changed. You might want to run the image as a container, which runs your data processing code specified in the CMD of the Dockerfile: docker run my-pipeline:0.1 Which should hopefully get as far as talking to AWS before failing. You can ‘shell into’ the linux container to take a look for debug purposes: docker run -it my-pipeline:0.1 bash And if bash is not available, try sh. 6.4.5 Build the image Concourse will: build your GitHub repository into a Docker image upload the Docker image to a private location in AWS Elastic Container Registry creates the AWS Role For more about the build see: Build and deploy guidance 6.4.6 Airflow DAG You need to define a DAG of your Tasks, so that Airflow can run it. A DAG is defined in its own .py file in the airflow-dags repository. Normally you would clone the repo, add the new DAG file, commit it to a branch and create the Pull Request. You can achieve this in R Studio, Jupyter or on your local machine if you have git, and then use GitHub to create the Pull Request. Alternatively you could simply click the “Create new file” button in GitHub and use the web interface to edit it and create a pull request in one go! The airflow-dags repo has protected master branch, so it requires a review from the data engineering team before you can merge. Ask for a review from moj-analytical-services/data-engineers. Use an existing DAG as an example: https://github.com/moj-analytical-services/airflow-dags/blob/master/mags_curated.py The image is the address of the Docker image - look in the Concourse build log: successfully tagged ... Tip: Before you commit, run the file with python to check the python syntax is ok. If it gets as far as giving you import errors then you know the python syntax is ok. (If you can be bothered to install the python libraries you could run it all) Merging the Pull Request needs to be done by the ASD Data Engineering team, to ensure the IAM permissions are acceptable. The DAG will appear in the Airflow web interface a couple of minutes after the Pull Request is merged. 6.4.7 Airflow web interface The Airflow web interface allows you to see the data pipelines running, view the logs, see past runs, etc. Link: https://airflow.tools.alpha.mojanalytics.xyz You can manually start (“trigger”) the pipeline with this button: "],
["build-and-deploy.html", "Part 7 Build and Deploy 7.1 deploy.json 7.2 Starting a build/deploy", " Part 7 Build and Deploy Concourse is the Analytical Platform service that automatically builds and deploys R Shiny apps, Web apps and Airflow data pipelines. This is also known as ‘continuous integration’ and ‘continuous deployment’ (CI/CD). 7.1 deploy.json Concourse automatically scans git repositories in the moj-analytical-services Github organisation to find repos that are ready to build/deploy. It does this by checking whether repositories contains a ‘magic’ file (on the master branch) that controls the build/deploy: deploy.json. The scan happens every 24 hours, or sooner if a repo in moj-analytical-services is tagged with a release on GitHub (or if it is urgent it can be triggered by an admin). For the format of deploy.json, see the relevant subject: R Shiny app Web app Airflow pipeline Changes to deploy.json only take effect when committed to GitHub (master branch), a Release is created and the deploy is successful (see Scan organisation and deploy). 7.2 Starting a build/deploy To trigger a Concourse build/deploy, make sure your code is committed and pushed to GitHub and then create a release on GitHub. Call the first release v0.0.1, then v0.0.2 etc. The version should always go up in number, or it won’t build. It’s good practise to use semantic versioning. Your task appears in Concourse within 30 seconds. Find it in Concourse to start it as follows: Open Concourse: https://concourse.services.alpha.mojanalytics.xyz/ Select team ‘main’. Click ‘login with ap-main’ followed by your GitHub login and possibly 2FA for GitHub and possibly 2FA for AP. Click the hamburger icon: (top-left) to see the list of repos that can be built. Click your repo name to see its diagram with the big ‘Deploy’ box in the middle, inputs and outputs. Click on the ‘deploy’ box to see details of its builds. You can see previous builds (numbered - e.g. “8 7 6 5 4 3 2 1”). The colour of the build number box gives the status: Grey = paused or pending Yellow = in progress Blue = paused Green = built ok Red = build failed Brown = aborted Orange = errored To start the job (this first time): Click play (on the left, against your repo name) AND the “+”: It should go grey (pending) After 30 seconds it should got yellow (building). If it does not go yellow, check the task is not paused (and possibly the job itself is paused) - press play if is showing. It will build/deploy, taking 5 minutes to an hour. You can track progress and see any errors like this: Subsequent build/deploys are started about 5 seconds after doing a GitHub release (unless someone pressed the pause button on the Concourse task). You have to refresh Concourse in your browser to see a fresh build. "],
["acceptable-use-policy.html", "Part 8 Acceptable use policy 8.1 Scope 8.2 Who this policy applies to 8.3 General principles 8.4 GitHub", " Part 8 Acceptable use policy 8.1 Scope This acceptable use policy covers the use of the Analytical Platform and all associated software and applications. This policy applies in addition to the MoJ acceptable use policy. 8.2 Who this policy applies to This policy applies to all users of the Analytical Platform (excluding users of apps hosted by the platform). 8.3 General principles All users will: report any security incidents, including a loss of data, in line with the relevant MoJ, HMPPS or HMCTS procedures; report any breach of this acceptable use policy to the Analytical Platform team; follow all relevant information governance procedures; protect their login credentials appropriately; create secure passwords following best practice guidelines (see here); ensure that two-factor authentication is enabled when accessing GitHub and the Analytical Platform (see here the MoJ security guidance about multi-factor authentication); sign out of the Analytical Platform when access is not required; understand they and MoJ have a legal responsibility to protect personal and sensitive information; understand that their use of the Analytical Platform may be monitored; ensure that all transfers of data onto and within the Analytical Platform are conducted safely and securely; not access the Analytical Platform from any non-MoJ IT system, such as a personal computer; not share their account or login credentials with any other person; not use the same login credentials for more than one system or purpose; not store any data on the Analytical Platform that is classified as SECRET or TOP SECRET; not move any data to the Analytical Platform without completing a data movement form; not attempt to access any data, apps or software on the Analytical Platform without the appropriate permission; and not use the Analytical Platform to undertake any illegal activity or any activity that could harm MoJ’s reputation or compromise the security of data or IT systems. App admins and data source admins will: ensure that app and data source users have the correct read/write permissions; ensure that app and data source users only have access to the minimum data required for them to perform their job; and regularly review access permissions for app and data source users, including when users join or leave MoJ, or move within MoJ. 8.4 GitHub In almost all cases, work must be stored in private repositories in the MoJ Analytical Services organisation. You may only store work in a public repository if you have: verified that the work contains no sensitive information or secrets; obtained prior written permission from your line manager; and followed the guidance on making source code open and reusable. GitHub may be used to store: source code; reports and documentation; and small, non-sensitive data sets (on a temporary basis when alternatives such as S3 are not practical) in accordance with the following restrictions. All users will: not store any large data sets (&gt; 1,000 records) in GitHub; not store any data, source code or documentation containing sensitive information in GitHub; not store any data, source code or documentation containing personal information in GitHub; not store any credentials or secrets, such as usernames, passwords, database connection strings or API keys in GitHub; provide access to private repositories on a need-to-know basis; store all MoJ work in the MoJ Analytical Services organisation; not store any work in public repositories without obtaining prior written permission from their line manager; verify that any work stored in public repositories does not contain any sensitive or personal information; and follow the guidance on making source code open and reusable. "],
["information-governance.html", "Part 9 Information governance 9.1 Data management 9.2 Data movements 9.3 Data Offshoring 9.4 Reporting security incidents", " Part 9 Information governance 9.1 Data management 9.1.1 Permissions and access Access to data and apps on the Analytical Platform should be provided on a need-to-know basis. If you are an admin for data source, you can control which users have access from the Analytical Platform control panel. Some app access permissions can be specified in deploy.json (see Section ??), however, users cannot manage an app’s user access list themselves. DOM1 and MacBook users can request access to a data source (for themselves, other users or customers) on the #ap-admin-request Slack channel. Quantum users should email the Analytical Platform team. If requesting access to a data source, you should provide the GitHub usernames of the relevant users. If requesting access to an app, you should provide the email addresses of the relevant users. 9.1.2 Data retention 9.1.2.1 Amazon S3 Files stored in Amazon S3 are retained indefinitely until they are deleted. Files stored in Amazon S3 are backed up automatically. Once files are deleted from Amazon S3, they are deleted permanently along with all backups and cannot be restored unless the bucket is versioned. Versioning is disabled by default. 9.1.2.2 Home directory Files stored in users’ home directories are retained indefinitely until they are deleted. Files stored in users’ home directories are backed up to Amazon S3 automatically. Previous versions of files stored in users’ home directories are also backed up to Amazon S3. Once files are deleted from a user’s home directory, the backup is retained for a further 90 days and can be restored. To request that a file be restored or to access a previous version of a file, contact the Analytical Platform team. 9.1.2.3 Best practice guidelines All users should comply with the following best practice guidelines for retaining personal data. All users will: be aware when they are working with personal data; consider and justify why personal data needs to be retained and for how long; identify when personal data needs to be kept for public interest archiving, scientific or historical research, or statistical purposes; review the need to retain personal data on a regular basis; erase or anonymise personal data when it is no longer required; and erase data as required in accordance with an individual’s ‘right to be forgotten’ under the Data Protection Act 2018. The relevant information asset owner (IAO) should be able to advise if any additional data retention policies or requirements apply. 9.2 Data movements All data movements should take place safely and securely to ensure that data is protected at all times, including when in transit. To facilitate this, when moving any data onto the Analytical Platform, you must complete a data movement form. The data movement form can be found here. Guidance on completing the form can be found in Section 9.2.1. Depending on the details of the data movement, you may also be required to complete or update: A technical migration form; Data protection impact assessments (DPIAs); Privacy impact assessments (PIAs); Privacy notices; or Information management and asset logs. Before completing a data movement form, you should ensure that you have already obtained all necessary approvals and completed all required supplementary documentation. 9.2.1 Data movement form guidance You may not be required to complete all of the questions below depending on the details of your data movement. 9.2.1.1 Contact information Name Email address Phone number Is the main person carrying out the data movement the same as the requestor? Name of the main person carrying out the data movement Email address of the main person carrying out the data movement Phone number of the main person carrying out the data movement 9.2.1.2 The data What data is being moved? What fields does the data contain? What is the security marking of the data? The Analytical Platform is suitable for data classified as OFFICIAL and OFFICIAL-SENSITIVE. SECRET and TOP SECRET data is not allowed on the Analytical Platform. Information on security markings and classifying information can be found here. Will the data be modified or changed in order to mask sensitive content or personal information? Guidance on masking and anonymisation can be found in sections 12.4.1 and 12.4.2. How will the data be changed or modified in order to mask sensitive content or personal information? Will the data be modified or changed before or after being moved to the Analytical Platform? 9.2.1.3 Personal data Does the data include personal data? Personal data is information that can be used to identify a person directly or indirectly in combination with other information. Personal data includes names, identification numbers, addresses and online identifiers. Guidance from the Information Commissioner’s Office (ICO) on identifying personal data can be found here. Further information on handling personal data in MoJ can be found here. 9.2.1.4 Data controller Is MoJ the data controller responsible for the data? A data controller is an entity registered with the Information Commissioner’s Office (ICO) that exercises overall control over the purposes and means of the processing of personal data. MoJ is the controller for MoJ HQ, HMPPS, HMCTS, LAA, OPG and some other agencies and public bodies. The Analytical Platform guidance contains a list of agencies and arm’s length bodies that are data controllers in their own right. The relevant information asset owner (IAO) should also be able to advise who the correct data controller is. Who is the data controller responsible for the data? The data controller could be another MoJ agency or public body (for which MoJ is not the responsible controller), another government department or a third party. You can use the ICO Data Protection Register to determine whether an entity is a controller. The following agencies and public bodies are data controllers: Criminal Injuries Compensation Authority (CICA) Children and Family Court Advisory and Support Service (CAFCASS) Criminal Cases Review Commission (CCRC) Legal Services Board (LSB) Parole Board for England and Wales Youth Justice Board for England and Wales (YJB) Civil Justice Council (CJC) Family Justice Council (FJC) Sentencing Council for England and Wales Office for Legal Complaints (Legal Ombudsman for England and Wales) The Official Solicitor to the Senior Courts The Public Trustee Prisons and Probation Ombudsman (PPO) Is a suitable data sharing agreement in place with the data controller? Is the controller aware of the data movement and the intended use of the Analytical Platform as a data processor? 9.2.1.5 Data protection and privacy Have the relevant Data Protection Impact Assessments (DPIAs) and Privact Impact Assessments (PIAs) been created or updated and reviewd by the data protection team? If you are unsure whether a DPIA or PIA already exists or needs to be created, you should contact the relevant Information Asset Owner (IAO). Further information on DPIAs and PIAs can be found here. You can also contact the Data Protection Officer mailbox here. Have the relevant privacy notices been updated to reflect use of the Analytical Platform? A privacy notice is required to let customers using a product or service know: who is collecting the data; what data is being collected; if they are required to provide the data; what is the legal basis for processing the data; how the data will be used; if the data will be shared with any third parties; how long the data will be held for; if the data will be used for automated deecision-making, including profiling; what rights they have in relation to their data; and how they can raise a complaint about the handling of their data. Further information on data privacy and privacy notices can be found here. ICO guidance can also be found here. 9.2.1.6 The data movement Why is the data being moved? What is the source of the data? Where will the data be stored on the Analytical Platform? Which S3 bucket will the data be stored in? How many records does the data contain? Who will have access to the data on the Analytical Platform? How will the data be moved? Describe how the data will be moved from the source location. Will the data be moved electronically, by email or by physical media? Will it be uploaded manually to S3 or a home directory or will there be a direct integration with another system? Is a technical migration form required? A technical migration form is not required for one-off data movements of static files (such as .zip files and .csv files) that are carried out manually (i.e. by email or direct upload to S3 or a home directory). A technical migration form is required for all other data movements. If you are unsure whether your data movement requires a technical migration form, please contact the Analytical Platform team. A template technical migration form can be found here. To access this template, you must be signed in to Office 365. If you are unable to access the template, please contact the Analytical Platform team. Technical migration form Please send your form as an attachment to the Analytical Platform team. Is this a one-off or recurring data movement? When is the data movement expected to occur? What is the intended frequency of the data movement? When is the first data movement expected to occur? Will the data movement be in place for a fixed period of time? When will the final data movement occur? When will the data movement request be reviewed? Recurring data movements that occur indefinitely should be reviewed at least every two years. 9.2.1.7 Data retention How long will the data be retained on the Analytical Platform? Is this in line with departmental and Analytical Platform data retention policies? Guidance on data retention can be found in Section 9.1.2. 9.2.1.8 Information Asset Owner (IAO) All data in MoJ is assigned an Information Asset Owner (IAO). The IAO is responsible for the registration, labelling, storage, transfer and retention of that data. You should seek approval from the relevant IAO for all data movements. Does the data movement require approval by an Information Asset Owner (IAO)? IAO name IAO email address Has the IAO approved this data movement? 9.2.1.9 Senior Information Risk Officer (SIRO) Information on the responsibilities of SIROs can be found here. SIRO approval may be required depending on local information assurance policies. SIRO approval is usually required when a data movement involves non-routine risks. The relevant IAO should be able to advise if SIRO approval is also required. Does the data movement require approval by a Senior Information Risk Officer? SIRO name SIRO email address Has the SIRO approved this data movement? 9.2.1.10 Information assurance Have you discussed the data movement with the relevant information security team(s)? The relevant information security teams are: MoJ HQ – cybersecurity team HMCTS – information assurance team HMPPS – information management and security team Have you considered all other information assurance requirements, including, but not limited to, those arising under the Public Records Act, the Official Secrets Act, the Data Protection Act and the Freedom of Information Act? 9.2.1.11 Confirmation Have you read and understood the Analytical Platform guidance and acceptable use policy? 9.2.2 Data Minimisation It is good practice to undertake data minimisation when moving data to the Analytical Platform as all software imposes memory limits. Data minimisation is also a princicple of GDPR. Any personal data used should be: adequate; relevant; and limited to what is necessary. In practice, you may want to consider whether a subset of a larger data set (removing irrelevant fields or observations) is sufficient for your analysis. 9.3 Data Offshoring The Analytical Platform is hosted on Amazon Web Services (AWS) (a US company part of the Amazon group) with primary data storage and processing locations in their European regions. 9.4 Reporting security incidents As soon as you become aware of an actual or potential security incident, including a loss of data, you should follow the guidance here and here. "],
["how-to-get-support.html", "Part 10 How to get Support 10.1 Summary 10.2 Intro 10.3 Routes of Support 10.4 How to ask for support: Creating a Reproducable Example 10.5 Raising Issues", " Part 10 How to get Support 10.1 Summary The Analytical Platform team in MOJ Digital &amp; Technology is responsible for providing access to software like R Studio and Jupyter. Analysts themselves are responsible for the code they write in these tools, and the Platform team is not responsible for assisting with problems with this code. As a rule of thumb, if the problem you’re experiencing would also occur with R Studio or Python installed on a standalone computer, then the platform team don’t offer support. If you’ve read through the user guidance and are still stuck, the best place to go for support is the analytical-platform Slack channel for issues with the platform, or the R channel and python channel to get support from peers in your code. What follows provides further details about how to ask for support, and what you can do to get your issues resolved as quickly as possible. 10.2 Intro When using the analytical platform you may experience problems with administration and the platform infrastructure (such as logging in and authorisation) issues with your code (running R or Python scripts) or the platform not functioning as expected, each of these will have a different support path as set out below. For every problem though there are a few quick steps you can try that will make it easier to provide help and may even fix your issue outright. With the exception of direct administration issues such as resetting authorisation try to: Reproduce the issue: if you can reliably recreate the issue time after time this will make it easier to pin point exactly what is going wrong. This may not always be possible with intermittent faults. Be specific: isolate the problem as much as possible and narrow it down to the smallest segment. It’s much easier and quicker if you can pin point a function or line of code, or the exact step that is causing a failure. Check if it is a known issue: it could be that the problem is experienced regularly by others or is a known bug currently being fixed. The place to go to check this will vary on the problem but checking Github issue logs, googling the problem, checking stackoverflow and browsing the Slack channels can often highlight the best next steps to take. 10.3 Routes of Support 10.3.1 Platform Support The MoJ Digitech team is responsible for supporting problems with the underlying analytical platform. If there is a issue relating to the running of the platform or surrounding infrastructure then they can provide support. Examples of this include: Bugs or unexpected issues Platform crashing or inaccessible Deployment of Shiny apps not working Administration of accounts Reset authorisation Access priviledges not working as expected This does not include any problems which occur as a result of the limitations or incorrect use of software by users on the platform. If issues occur whilst running code or using version control please see the sections below. Remember the first port of call for using the platform should always be this user document. The MoJ Digitech team have a Slack channel where you can post questions and a Github page where you can raise a formal issue ticket. For only those users who cannot access slack or Github (i.e. some non DOM1 users) there is a direct email address which can be used: analytical_platform@digital.justice.gov.uk - but this will be monitored for emails from non DOM1 users. We direct users to Slack for support because that means that users can quickly see whether similar issues are being faced by others, and Slack provides a more efficient way of getting the information we need from users to fix their problems. 10.3.2 Coding Support (R &amp; Python) There is no team who is responsible for offering support, but we have an active community of colleagues who offer support voluntarily. Users are responsible for the code they create and are expected to debug and solve problems themselves. If you need support, there are a multitude of resources: This platform user guidance document contains a number of different sections explaining how the platform interfaces with R and common issues that may occur. There is an R slack channel which is visited frequently by R users from across ASD, please use this forum to ask questions or post examples of code, putting your problem out in the open is also a great way to help others who may be experiencing the same thing. Stack Overflow and Google are powerful tools when you encounter a problem - it’s very unlikely that you are the first to come across it. Stack Overflow also has the added benefit of allowing you to sign up and post questions publicly, massively widening the chance of finding someone who can help. Be aware not to publish data with questions and be sure to follow the guidance of reproducable examples below. An ASD R training group also exists who can help getting you up to speed with R (unfortunately there isn’t yet one for Python), this is led by Aidan Mews who can be contacted to discuss the option of formal R training or guidance on online based tutorials. If you’ve explored the above options and are still having problems, then the Data Science Hub (DaSH) team will try to provide assistance, although please be aware that team members are under no obligation to do so and appreciate the time this may take away from their other work. Data science team members are active contributors to the Slack channels and creating a post there is the preferred method of contact. 10.3.3 Git Support (Version control) Similarly using git and interfacing with Github are processes that users are expected to learn and manage themselves. Git and Github are hugely beneficially to analysts as they provide a single version of the truth, incorporate continual QA and allow multiple users to work consectutively on a single file (just to name a few of the benefits). The set up and configuration of Git is covered in this guidance but ff you have any further queries related to the use of git or Github there are the following routes of support: There is a Git/Github training course being run by the Data Science Hub which will introduce you to the key concepts and provide you a practical example of how it works. It is highly recommended you take this course when first starting out with git. To find out about when the next session is happening, or ask question about git, you can use the data science slack channel. 10.4 How to ask for support: Creating a Reproducable Example When posting a problem or question on a forum (whether this be the ASD slack channel or a public website such as Stack Overflow) it is vital to ensure that other users can reproduce your example, and therefore understand your issue. Posting a question with no example or an example which is too large will prevent otheres from being able to help effectively. When creating a minimal reproducable example: Isolate the section of code which is causing the problem, ensure that the example can be run without the need for any prior processing or code. Remove any real data, helpers cannot be expect to have access to the same dataset so providing a small dummy dataset is important. In R you could use the built in datasets such as mtcars or iris which all R users have access to. Provide the expected output, be clear as to what you are trying to achieve and what the correct outcome will look like. Details on how to produce a good reproducible example can be found here: https://stackoverflow.com/help/mcve https://github.com/tidyverse/reprex 10.5 Raising Issues Rather than posting questions on Slack you could also visit the analytical platform Github page and raise a formal issue ticket. This will then flag your issue directly to the MoJ Digitech team who will respond through the issue log on Github. There is a fixed format for raising issues which should be followed to ensure the team can help: https://github.com/ministryofjustice/analytics-platform/blob/master/.github/ISSUE_TEMPLATE.md This approach of raising issues also applies to any package, model or project that has a Github page. These issue logs are the most common method for highlighting problems in open source project, many R packages for example have a Github page where you can raise issues with the developer or search to see whether your problem has already been raised. "],
["common-errors-and-solutions.html", "Part 11 Common Errors and Solutions 11.1 Failed to lock directory 11.2 rsession-username ERROR session hadabend 11.3 Status Code 502 error message 11.4 Unable to access data using aws.s3 package. 11.5 s3tools::s3_path_to_full_df() fails on Excel file 11.6 Two-factor authentication problems 11.7 I’m having problems deploying a Shiny app", " Part 11 Common Errors and Solutions 11.1 Failed to lock directory This error is typically encountered after a failed package install. Error ERROR: failed to lock directory ‘/home/robinl/R/library’ for modifying Try removing ‘/home/robinl/R/library/00LOCK-readr’ Solution Run the following: install.packages(&#39;pacman&#39;) pacman::p_unlock() If that does not work, or if you have trouble installing the pacman package, try the following: Go to Tools -&gt; Shell and type: rm -rf /home/robinl/R/library/00LOCK-readr See here for more details. Be careful with the rm command! 11.2 rsession-username ERROR session hadabend Errors like the following can typically be ignored: [rsession-aidanmews] ERROR session hadabend; LOGGED FROM: rstudio::core::Error {anonymous}::rInit(const rstudio::r::session::RInitInfo&amp;) /home/ubuntu/rstudio/src/cpp/session/SessionMain.cpp:1934 They seem to occur when R Studio has been unable to restore your session following a crash. Note the items in your R environment will no longer be there following a crash, and you’ll need to re-run your scripts to bring your data back into memory. Crashes often occur when you run out of memory. For now, you can use pryr to track your memory usage - see here. We are working on giving users greater visibility of their memory usage. 11.3 Status Code 502 error message R Studio Server is single-cpu (single thread), which means it can’t ‘do two things at once’. If you ask it to, sometimes one of the operations will timeout. The ‘Status code 502’ message is basically a timeout message. Usually this doesn’t cause anything to crash. You just need to wait for the currently-running code to finish executing and try again. One example of when this can happen is if you attempt to save a file whilst a long-running script is running. R Studio has to wait until the script has finished running to attempt to save the file. However, sometimes the wait is too long, causing a timeout. In this case, you just need to wait for the code to finish running, and then press save again. 11.4 Unable to access data using aws.s3 package. Unfortuntely aws.s3 does not support the granular file access permission model we are using on the platform. Specifically, it is unable to automatically provide the user with the right file access credentials. We provide s3tools as a solution to this problem, which manages your credentials for you. We recommend that, where possible, users should use s3tools. Where this is not possible, include a call to s3tools::get_credentials() prior to making the call to aws.s3, and this will guarantee that fresh credentials are generated before your call to aws.s3 11.5 s3tools::s3_path_to_full_df() fails on Excel file s3tools::s3_path_to_full_df attempts to read in data from various filetypes, including Excel, but this sometimes fails. If it does, you have two options: 11.5.0.1 Option 1: Use s3tools::read_using() This allows you to specify what function you want to use to attempt to read the file. So, for example you can do: s3tools::read_using(openxlsx::readWorkbook, path = &quot;alpha-everyone/my_excel.xlsx&quot;) to attempt to read the file alpha-everyone/my_excel.xlsx using openxlsx::readWorkbook 11.5.0.2 Option 2: Save the file to your project directory and load it from there, rather than from S3 s3tools::get_credentials() aws.s3::save_object(&quot;my_excel.xlsx&quot;, &quot;alpha-everyone&quot;, &quot;file_name_to_save_to_in_home_directory.xlsx&quot;) and then read it in using e.g. openxlsx::readWorkbook(&quot;file_name_to_save_to_in_home_directory.xlsx&quot;) Note, it’s best to avoid using aws.s3 directly, see here 11.6 Two-factor authentication problems Two-factor authentication (2FA) is critical to the security of the platform. We have opted to use smartphone-based authentication apps as hardware tokens, like the RSA device you use to log in to DOM1, are expensive and SMS codes are susceptible to interception. Note that there are two layers of 2FA in action on the platform: Your GitHub account must have 2FA enabled. When you log in to GitHub, your session will stay active for a month before you need to re-enter your 2FA code. Your GitHub username identifies you to the platform, and we use this identity to control access to data and other resources once you’ve logged into the platform. You therefore must be logged into GitHub to use the platform. Your Analytical Platform account has a separate 2FA step. You will be prompted to set this up the first time you access the platform and on subsequent uses, depending on the machine you use and the network it’s connected to: From a corporate-networked machine (DOM1 or QUANTUM) or corporate wifi (e.g. MoJDigital) then you will not be challenged for this. Otherwise (e.g. working from home on a non-corporate-networked machine) you’ll be challenged to provide the 2FA code during every sign-in to access each part the platform: Control Panel, R Studio, Grafana etc. This security step lets you log into the platform and use it. Whilst you may be prompted to enter your platform 2FA frequently (e.g. when working from home), but you will not need to enter your GitHub 2FA because this is remembered for a month. However, if you have not logged into the platform for more than a month, you will first have to login to GitHub (and enter your GitHub 2FA code), and you will then also be prompted to enter your platform 2FA code. 11.6.1 I’ve lost my platform 2FA If you’ve lost your platform 2FA code because e.g. you’ve broken or lost your phone, please contact the Analytical Platform team and we will reset it for you. 11.6.2 I have entered my 2FA code, but the platform will not accept it Smartphone based 2FA apps require the phone’s clock (the time) to be up to date. If your phone’s clock is out of sync by more than 30 seconds or so, this can cause the 2FA codes to be out of sync. Most phones syncronise their time with the network provider, so this is not a problem. If your time is out of sync, you need to navigate to your clock settings in your phone, and enable the option to sync the time. See e.g. here 11.7 I’m having problems deploying a Shiny app For help resolving deployment problems, see the advanced deployment section of the docs. "],
["annexes.html", "Part 12 Annexes 12.1 Memory limits 12.2 What are the benefits of Github and why do we recommend it? 12.3 Step by step guide to setup Two Factor Authentication 12.4 Data Minimisation", " Part 12 Annexes 12.1 Memory limits All the software running on the Analytical Platform has its memory controlled. Each running software container has a minimum and maximum amount available. Here’s an explanation of the key terms: ‘memory’ is needed for your app and code, but the limiting factor is usually the data that you ‘load into memory’. Data is in memory if it is assigned to a variable, which includes a data frame. a ‘container’ is for one user’s instance of a tool (e.g. Sandra’s R Studio or Bob’s Jupyter) or app (e.g. the PQ Shiny App or the KPIs Web App). ‘Minimum memory’ is the amount the container is guaranteed. It is reserved for it. ‘Maximum memory’ may be available, but your container is competing for this memory with other containers on the server it happens to be running on. If you try to use more than is available or more than the Maximum then your container will be restarted. e.g. Sandra runs her code in R Studio, which loads 8GB of data into a data frame, and she sees in Grafana that this takes 10GB of memory. This is above the minimum of 5GB, and because the servers aren’t too busy the extra 5GB are available. Later the server that her R Studio container is running on happens to get full and when she tries to do something that needs another 1GB memory (i.e. total 11GB) she finds that R Studio restarts, interrupting her work for a few minutes. She restarts her code, but because her R Studio now happens to be running on a different server that is less busy, she finds she can run her code and use 11GB fine. You can monitor your memory usage using Grafana (there is also a link from the Control Panel). Scroll down to “User RAM usage” and you can click on your name to just show yours. Also useful is to adjust the time period from being the “Last 1 hour”, by clicking on the label at the top-right corner. Current memory limits: Container type Minimum Maximum R Studio 5 GB 20 GB Jupyter 1 GB 12 GB App no limit no limit You can work on a dataset that is bigger than your memory by reading in a bit of the data at a time and writing results back to disk as you go. If you’re working on big data then consider taking advantage of tech like Amazon Athena or Apache Spark, which are available through the Analytical Platform too. Our laptops tend to have only 8GB or 16GB, so it’s an advantage of the AP that we can offer more. We are open to increasing the maximum memory, so let us know if you need more, to help us make a case for covering the additional cost. 12.2 What are the benefits of Github and why do we recommend it? Github is a central place to store our analytical projects - particularly those which are built primarily in code. Github keeps track of who wrote what, when, why they wrote it, why we can trust its correctness, and which version of the code was run to produce an analytical result. This is useful if you’re work on your own, but the benefits are greatest when you’re working in a team. Here is some more details of what Git offers: It provides a single, unambigous master version of a project. No more model_final.r, model_finalv2_final.r. etc. It enables you and collaborators to work on the same project and files concurrently, resolving conflicts if you edit the same parts of the same files, whilst keeping track of who made what edits and when. It enables work in progress to be shared with team members, without compromising the master version. You never get confused between what’s not yet final, and what’s trusted, quality assured code. The work in progress can be seemlessly merged into the master version when it’s ready. It provides a history of all previous versions of the projects, which can be meaningfully tagged, and reverted to (undo points). e.g. we may wish to revert to the exact code that was tagged ‘model run 2015Q1’. It reduces the likelihood of code being lost in messy file systems, such as on DOM1. Files sits within defined project ‘repositories’, with all code accessible from this location. It provides an extremely powerful search function. The ability to search all code written by Ministry of Justice analysts in milliseconds. Or all code written by anyone, for that matter. It enables an easier, more robust, more enjoyable approach to quality assurance. In particular, it offers the potential to continuously quality assure a project as it’s built, rather than QA being an activity that’s only done once the work is complete. For example, all additions to the codebase can be reviewed and accepted by a peer before being integrated into the master version. It includes productivity tools like Github issues (essentially a tagged to-do list), and a trello style workflow (Github projects), with automation. Git stores a huge amount of meta data about why changes were made and by whom. This dramatically reduces the danger of code becoming a ‘black box’. The to-do list is automatically linked to the implementation - e.g. The issue of ‘improve number formatting’ is automatically linked to the specific changes in the code that fixed the number formatting. It makes it much easier to build reusable components, and make parts of our code open source (available to the public). For example, we use R code written by statisticians around the world that’s been put on Github, and we know that people across government have been using some of our R code. We can collaborate easily with other government deparments and anyone else for that matter. It makes it easier to ask for help with your work, particularly with colleagues who are working remotely. You can hyperlink and comment on specific lines of code. You can write rich, searchable documentation - e.g. this user guide is hosted on Github! Finally, we have chosed git specifically because it seems to be by far the most popular version control system - see here and here 12.3 Step by step guide to setup Two Factor Authentication Two factor authentication (2FA) is critical to the security of the platform. We have opted to use smartphone based 2FA apps due to the expense of giving out hardware tokens like the RSA device you use to log into DOM1. Note that there are two layers of two factor authentication (2FA) in action on the platform: Github Account 2FA Your Github account must have 2FA enabled. When you log in to Github, your session will stay active for a month before you need to re-enter your 2FA code. Your Github username identifies you to the platform, and we use this identity to control access to data and other resources once you’ve logged into the platform. You therefore must be logged into Github to use the platform. Analytical Platform 2FA Your Analytical Platform account has a separate 2FA step. You will be prompted to set this up the first time you access the platform. This code must be entered once a day. This security step lets you log into the platform and use it. Usually, when you log into the platform, you will be prompted to enter your platform 2FA, but you will not need to enter your Github 2FA because this is remembered for a month. However, if you have not logged into the platform for more than a month, you will first have to login to Github (and enter your Github 2FA code), and you will then also be prompted to enter your platform 2FA code. 12.3.1 Step by step - logging into the platform for the first time The first time you log into the Analytical Platform, you will be asked to set up 2FA. Your welcome email will direct you to the platform Control Panel. 12.3.1.1 Step 1: Log into Github to identify yourself to the Analytical Platform If you’re already logged into Github, you will not see the ‘Sign in to GitHub to continue to Analytics platform’ screen. 12.3.1.2 Step 2: Set up your Platform 2FA using your smartphone In this step, you set up the second layer of 2FA, your Platform 2FA. Scan the code using your smartphone app, and enter the code that comes up on your smartphone. Note: If you get the error ‘Wrong or expired code’, you need to make sure that your phone’s clock is accurate. See here 12.3.1.3 You’re now done Once you’re entered your platform 2FA code in the interface above, you should now have access to the platform. You will need to enter your platform 2FA code around once a day as you use the platform. 12.4 Data Minimisation In order to reduce cybersecurity/information risks to data and promote/maintain a privacy-centered approach you should consider how data can be minimised or obfuscated before being imported if you can do so while maintaining analytical usefulness and data integrity. 12.4.1 Pseudonymisation Pseudonymisation is “…the processing of personal data in such a manner that the personal data can no longer be attributed to a specific data subject without the use of additional information…” An example of pseudonymisation would be replacing the name of an individual with a unique hashed value. This would preserve the concept of an individual person but would mean their name is not stored in the Analytical Platform when it is not directly needed. This is pseudonymisation as the Analytical Platform with the data that it now holds can no longer identify that person without other data that it likely does not have. 12.4.2 Anonymisation Anonymisation is “…information which does not relate to an identified or identifiable natural person or to personal data rendered anonymous in such a manner that the data subject is not or no longer identifiable [even when joined with other accessible data]…” Under current data protection legislation, true anonymisation is quite hard to achieve but should be considered. An example of anonymisation would be using summary values such as where there are 1000 unique ‘users’ in your database, the Analytical Platform held data only holds the ‘1000’ information, not each individual user record. This is anonymisation as the Analytical Platform with the data that it now holds can no longer identify any person at all, as it will be mathematically impossible to use the number ‘1000’ to identify any individual. "],
["conda-package-management.html", "A Conda Package Management A.1 Unified package management for both R and Python A.2 Installing Packages A.3 Platform limitations A.4 Environment Management", " A Conda Package Management A.1 Unified package management for both R and Python A.1.1 Faster builds CRAN1 mirrors do not provide linux2 compiled binaries for packages. This means a long wait when doing install.packages both in an R Studio session and when running a Docker build for a shiny application. A.1.2 Cross Language Both dbtools and s3tools rely on Python packages through the reticulate R to Python bridge. packrat only handles R dependencies, this means that packrat is not enough to reproducibly and reliably manage all of your application’s dependencies. conda supports managing both Python and R dependencies in a single environment. It can make sure all of these libraries are compatible with each other. A.2 Installing Packages If you need to find a package name you can use the anaconda search to find the name. Run conda install PACKAGENAME in the Terminal tab to install it. For more advanced usage have a look at the conda cheat sheet. A.2.1 R Most CRAN (around 95%) are available through conda, they have the same name as the CRAN package name with an additional r- prefix. This is to avoid clashes with Python packages with the same name. A.2.1.1 Examples: A.2.1.1.1 Install a package install.packages (in R-Console) conda install (in Terminal) install.packages('Rcpp') conda install r-Rcpp A.2.1.1.2 Install a specific version of a package install.packages conda install require(devtools) install_version(&quot;ggplot2&quot;, version = &quot;2.2.1&quot;, repos = &quot;http://cran.us.r-project.org&quot;) conda install r-ggplot2=2.2.1 A.2.2 Python Python packages do not require a prefix and can simply be installed using their name. A.2.2.1 Examples A.2.2.1.1 Install a package In the terminal run: conda install numpy which you can now access in your R session library(reticulate) np &lt;- import(&quot;numpy&quot;) np$arange(15) A.2.3 Operating System Packages Even if you want to continue using packrat to manage your R packages you can use conda to resolve operating system dependencies like libxml2. A.2.3.1 Examples A.2.3.1.1 Installing a package that relies on OS dependency Suppose you want to install the R package bigIntegerAlgos but it fails because it depends on a system level library called gmp. Switch to the terminal and use conda to install it. Then switch back to the R console and try to use install.packages again. A.3 Platform limitations Usually when using Conda, it makes sense to have one environment per project, but because we are using the Open Source version of R Studio, there is only a single Conda environment available. This means having to be careful to make sure packages don’t pollute your environment from another project. The following section explains how to manage your environments. A.4 Environment Management A.4.1 Reset your Environment to default Recommended before starting a new project. Will ensure that no unused dependencies are exported when you export an environment.yml for this project. conda deactivate; conda env remove -n rstudio; rm -rf ~/.conda/envs/rstudio/; conda create --use-index-cache --clone root -n rstudio --copy -y --offline; conda activate rstudio A.4.2 Exporting your Environment This is similar to making a packrat.lock file, it catalogues all of the dependencies installed in your environment so that another user can restore a working environment for your application. Check this environment.yml file into your git repository. conda env export | grep -v &quot;^prefix: &quot; &gt; environment.yml A.4.3 Making your R Studio Environment match an environment.yml When checking out a project that has an environment.yml do this to install any packages required by the project that you don’t have in your working environment. conda env update -f environment.yml --prune The “Comprehensive R Archive Network” (CRAN) is a collection of sites which carry identical material, consisting of the R distribution(s), the contributed packages and binaries.↩ Linux (Debian) is the environment that applications in the Analytical Platform run on.↩ "],
["access-local-ports-in-jupyter.html", "B Access Local Ports in Jupyter B.1 Running Plotly Dash apps B.2 Troubleshooting", " B Access Local Ports in Jupyter If you are developing a web application using Jupyter you will probably want to preview that web app before you deploy it. The Analytical Platform provides a special set of endpoints under /_tunnel_/&lt;portNumber&gt;/ that route to exposed HTTP ports on the localhost. There are a few things you need to keep in mind: &lt;portNumber&gt; can only be one from a limited range. By default, you can only route to ports 8050 and 4040–4050 inclusive. You should make your web app listen on one of those. The /_tunnel_/&lt;portNumber&gt;/ endpoint will only tunnel to services bound to a non–public IP address. By default, many web frameworks bind to host 127.0.0.1. You will need to change this to 0.0.0.0 or the tunnel won’t work. This is to prevent inadvertently exposing webapps to the tunnel. B.1 Running Plotly Dash apps B.1.1 Install dependencies In the terminal, install the Dash dependencies: pip install --user dash==0.39.0 # The core dash backend pip install --user dash-daq==0.1.0 # DAQ components (newly open-sourced!) The demo code in this document is known to work with these versions but you can install any version you like for your own code. If you are planning on turning this into a project, you’ll need to manage your dependencies by adding these to either requirements.txt and pip or environment.yaml and conda. B.1.2 Example code Save the Dash hello world app to a new python file called app.py: import dash from dash.dependencies import Input, Output import dash_core_components as dcc import dash_html_components as html import flask import pandas as pd import time import os server = flask.Flask(&#39;app&#39;) server.secret_key = os.environ.get(&#39;secret_key&#39;, &#39;secret&#39;) df = pd.read_csv(&#39;https://raw.githubusercontent.com/plotly/datasets/master/hello-world-stock.csv&#39;) app = dash.Dash(&#39;app&#39;, server=server, url_base_pathname=&#39;/_tunnel_/8050/&#39;) app.scripts.config.serve_locally = False dcc._js_dist[0][&#39;external_url&#39;] = &#39;https://cdn.plot.ly/plotly-basic-latest.min.js&#39; app.layout = html.Div([ html.H1(&#39;Stock Tickers&#39;), dcc.Dropdown( id=&#39;my-dropdown&#39;, options=[ {&#39;label&#39;: &#39;Tesla&#39;, &#39;value&#39;: &#39;TSLA&#39;}, {&#39;label&#39;: &#39;Apple&#39;, &#39;value&#39;: &#39;AAPL&#39;}, {&#39;label&#39;: &#39;Coke&#39;, &#39;value&#39;: &#39;COKE&#39;} ], value=&#39;TSLA&#39; ), dcc.Graph(id=&#39;my-graph&#39;) ], className=&quot;container&quot;) @app.callback(Output(&#39;my-graph&#39;, &#39;figure&#39;), [Input(&#39;my-dropdown&#39;, &#39;value&#39;)]) def update_graph(selected_dropdown_value): dff = df[df[&#39;Stock&#39;] == selected_dropdown_value] return { &#39;data&#39;: [{ &#39;x&#39;: dff.Date, &#39;y&#39;: dff.Close, &#39;line&#39;: { &#39;width&#39;: 3, &#39;shape&#39;: &#39;spline&#39; } }], &#39;layout&#39;: { &#39;margin&#39;: { &#39;l&#39;: 30, &#39;r&#39;: 20, &#39;b&#39;: 30, &#39;t&#39;: 20 } } } if __name__ == &#39;__main__&#39;: app.run_server(host=&#39;0.0.0.0&#39;) There are two important things to note here, where this code differs from the Plotly Dash example: The Dash class must be instantiated with a url_base_pathname. This should always be /_tunnel_/8050/ e.g. app = dash.Dash('app', server=server, url_base_pathname='/_tunnel_/8050/'). When you run the server, it must be bound to 0.0.0.0, e.g., app.run_server(host='0.0.0.0'). B.1.3 Run server from the terminal yovyan@jupyter-lab-r4vi-jupyter-7cb4bb58cf-6hngf:~$ python3 app.py * Serving Flask app &quot;app&quot; (lazy loading) * Environment: production WARNING: Do not use the development server in a production environment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:8050/ (Press CTRL+C to quit) B.1.4 Access via _tunnel_ URL Copy your jupyter URL and append /_tunnel_/8050/ to access your running Dash app. If your jupyter URL is https://r4vi-jupyter-lab.tools.alpha.mojanalytics.xyz/lab?, then your Dash app will be available at https://r4vi-jupyter-lab.tools.alpha.mojanalytics.xyz/_tunnel_/8050/. B.1.4.1 Who can access this /_tunnel_/ URL? Only you can access the URL and it can not be shared with other members of your team. It is intended for testing while developing an application. B.2 Troubleshooting This feature has been introduced in the v0.6.5 jupyer-lab helm chart. If following this guide doesn’t work for you it is likely that you’re on an older version. Contact us on the #analytical_platform Slack channel, alternatively, contact us by email to request an upgrade. "]
]
