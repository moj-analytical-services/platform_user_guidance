[
["index.html", "Platform Guidance About this guidance 0.1 What’s new?", " Platform Guidance MoJ Analytical Services 2019-04-17 About this guidance This guidance provides information about how the use the various features of the Analytical Platform. Please use the contents navigation bar on the left to find what you’re looking for, or click the magnifying glass icon above to search the guidance. Some online training recommendations are provided throughout. Platform is in early development The platform is being actively developed. We have given early access to the platform in order to receive feedback from the users. Please get in contact to tell us what you like, what you don’t like, and what doesn’t work. For bugs and problem reports, please raise a ticket via Github, here. For immediate support, contact us by email, or on the #analytical_platform Slack channel If you find any issues with the guidance, please report them here. 0.1 What’s new? May 2018: The control panel now includes a ‘data warehouse’ tab to manage data access. This replaces the previous approach using Github teams. April 2018: Substantial improvements to s3tools and s3browser. s3browser is now much faster, and s3tools has more functionality March 2018: Users’ R Studio ‘hibernate’ at 22:00 each evening, reducing overall resource utilisation Jan 2018: Users can restart their own R Studio in the event of a crash using the Control Panel "],
["quick-start.html", "Part 1 Quick Start 1.1 What is the Analytical Platform? 1.2 Why should I use the Analytical Platform? 1.3 FAQ: How do I? 1.4 Contacting us", " Part 1 Quick Start 1.1 What is the Analytical Platform? The Platform provides four main services to analysts: Access to analytical software. Data storage and access to datasets. The ability to create and share interactive data products and websites. Collaboration tools for analysts that enable teams to work concurrently on complex analytical projects, and perform QA incrementally. If you already have a platform account, you can access the platform here. 1.1.1 High level diagram of the platform: Click through from‘Analytical Tools’ tab  [Not supported by viewer] Shared data storage  in AWS [Not supported by viewer] Click through from ‘Warehouse data’ tab [Not supported by viewer] Control Panel [Not supported by viewer] The central portal from which you can access all platform services [Not supported by viewer] Deploy webapps suchas Shiny apps(currently superuser only) [Not supported by viewer] Click through from‘webapps’ tab(Superuser only) [Not supported by viewer] Access datausing s3browser Access data&lt;br&gt;using s3browser&lt;br&gt; User guidance and support [Not supported by viewer] Click throughfrom links infooter of ControlPanel [Not supported by viewer] Tip: You can click on the links in this diagram! &lt;i&gt;Tip: You can click on the links in this diagram!&lt;/i&gt; R Studio [Not supported by viewer] Github [Not supported by viewer] Store your code Store your code Grafana [Not supported by viewer] Monitor yourresource usage Monitor your&lt;br&gt;resource usage&lt;br&gt; Kibana [Not supported by viewer] View webapp logs View webapp logs 1.2 Why should I use the Analytical Platform? The Platform provides a number of advantages over existing analytical infrastructure: It provides access to the latest versions of modern analytical software, such as R Studio It provides more powerful computational resources (&gt;10Gb RAM and fast CPUs) than DOM1 machines. It can be accessed from multiple computer systems - DOM1, Quantum (HMPPS’s system), and MacBooks. It allows analysts to securely deploy data products such as R Shiny applications and web sites, including web-based data visualisations. The platform is hosted in the cloud, which makes it easy to access remotely. Data storage is unlimited, and computational resource can be scaled to demand. 1.3 FAQ: How do I? This section covers common tasks you may want to complete on the platform. Get access to the platform See Getting Started. Share data or code with a team TODO: Update following rewrite of section on data access groups If you’ve got a team but want to upload data see [Uploading data to S3][Uploading data to S3]. If you want to share code see guidance on Github. Upload and work with data If the data isn’t shared, and is less than 100 MB, you can upload it directly to R Studio. See ‘uploading files’ here. If you want to share the data, or if it’s more than 100 MB, you’ll need to set up a team, then upload data (see [Uploading data to S3][Uploading data to S3] ). To import data from S3 into R for analysis, see [Importing data from S3 into R][Importing data from S3 into R]. Make an interactive dashboard and deploy it securely See [Deploying a Shiny App ][Deploying a Shiny App ]. 1.4 Contacting us For quick support, contact us on the #analytical_platform Slack channel. Alternatively, contact us by email "],
["getting-started.html", "Part 2 Getting Started 2.1 Getting an account 2.2 Accessing the platform 2.3 Configuring Git and GitHub for use on the Analytical Platform 2.4 Guidance on writing code 2.5 Training Resources", " Part 2 Getting Started 2.1 Getting an account You use a specially authenticated GitHub account to log into the platform, which you can set up as follows: 2.1.1 Sign up to GitHub If you do not already have an account, head to https://github.com/ and sign up. When signing up, use all the default options, and make sure you save recovery codes as instructed. It’s good practice to choose a username that does not feature upper case characters. We recommend new users use their @justice email address. If you already have a personal GitHub account, you may also use that. You’ll receive an email with a verification link - you need to click this before using the platform. 2.1.2 Enable two-factor authentication on your GitHub account Once you’ve signed up to GitHub, you must enable two-factor authentication (2FA) on your GitHub account to enhance security. You can do this from your GitHub settings page. You are given the option of doing this using SMS, or an authentication app on your personal or work phone, such as Authy or Google Authenticator. We recommend that you use Authy for the following reasons: You can sync 2FA tokens across multiple devices, including mobiles, tablets and computers. You can create encrypted recovery backups in case you lose your device. You can use Touch ID, PIN codes and passwords to protect access to your 2FA tokens. You can still access secure websites when you don’t have phone signal. For step-by-step instructions on setting up 2FA, see the GitHub guidance for authentication apps or SMS authentication. For more information on the use of 2FA with the Analytical Platform and for troubleshooting tips, see Section 13.6. 2.1.3 Email us your GitHub username Once you’ve got a GitHub account setup, email your GitHub username to the shared mailbox (analytical_platform@digital.justice.gov.uk) and we will invite you to the private MoJ Analytical Services organisation. Once we’ve done this, you can accept the invitation by visiting the MoJ Analytical Services organisation, or by using the link in the email you’ll receive from GitHub. 2.1.4 Read the acceptable use policy Before you start using the platform, please read the acceptable use policy. 2.2 Accessing the platform Use the control panel to access R Studio and other parts of the platform. Using the control panel, you can also set up and manage team-based data access. 2.2.1 Accessing the platform You may use any formal MOJ technology solution such as DOM1, Quantum, MOJ Digital &amp; Technology or Workplace Technology laptops to access the platform. If you wish to access the platform from another formal HMG technology solution (for example, a laptop provided by another government department) you must ask first (analytical_platform@digital.justice.gov.uk). You must not access the platform from a non-MOJ device, such as a personal mobile phone or laptop. When you first access your platform account, you will be asked to set up additional two-factor authentication for the platform itself. You must do this using an authentication app on your personal or work phone, as no SMS option is available. For the reasons described in Section 2.1.2, we recommend that you use Authy. For step by step instructions on setting up this 2FA, see here. For troubleshooting 2FA issues, please see this section of the user guidance. 2.3 Configuring Git and GitHub for use on the Analytical Platform GitHub enables you to collaborate with colleagues on code and share you work with them. It puts your code in a centralised, searchable place. It also enables you to version control your work. In order to use GitHub, you need to set up a connection between your Analytical Platform account and GitHub. This needs to be done once, after which your Analytical Platform account will permanently be connected. It is worth doing because it avoids you having to repeatedly type in your username and password every time you interact with GitHub. This page covers only the initial setup. For guidance on how to use Git to sync your work with GitHub, please see here. You can find more detailed notes about setting up Git with R Studio here and here 2.3.1 Instructions The overview is as follows: Create an ‘SSH key’ within your platform R Studio or Jupyter. This is an unique code that is stored in your account. Having the key means you will not need to enter your password when interacting with GitHub. Register the key with GitHub. Configure git to use your username and email address. 2.3.1.1 Step 1 - create an SSH key You can create your SSH key two ways: 2.3.1.1.1 Method 1: using R Studio: Tools -&gt; Global options -&gt; Git/SVN -&gt; Create RSA key... You will be presented with dialog showing your key fingerprint, amongst other details. You do not need this information, therefore close the window: For the next step, you’ll need to copy and paste your SSH public key. You can copy this to the clipboard using the following dialogues: 2.3.1.1.2 Method 2: using Jupyter terminal: In Jupyter open a terminal by clicking the “+” then “Terminal”: Create a new one, putting in your own email address: ssh-keygen -t rsa -b 4096 -C &quot;my.name@justice.gov.uk&quot; When it asks for the filename and passphrase, just press Enter (leave them blank): Enter file in which to save the key (/home/jovyan/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: For the next step, you’ll need to copy and paste you SSH public key. First view it: cat /home/jovyan/.ssh/id_rsa.pub Then select it and copy it with cmd-C (Mac) or Ctrl-C (PC). It should ressemble this: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDsv0c89Bp7aCrMsSSy2en/aW89QRvJF75MZYxRN3As11KNFpYW0ocWxp5sKu5JA7b/h5Hg0Uv+SL11gN3DXZtYVFfX64iXIDoqAzfsdSWG1WaOSkUK3VyX8ByVhLWwisddfwPScQDOfipdYtQXg1574+HPZY6VTnQtksbgsSCULPml4kcEleX9evSBZRPAETBM5tlOTh8TVM2N+0r8G5eksd0F8T6byrXKj2+bRCVVUE5p5q5qryKd7cioOpE8fxU1wbB2Dg8fzsn5wAuySuOjRuuS5iecwfKIMxcIP/RoAimOdXdDMX7tI5wyaHWpLM3kKwYHQ3+JgEQPJPi37m2Ms29/ErZPf9Sa5/G6r2bA3wqz9A9o3z5BOxMGlXk73o1B3+oMGRFiFzefF/JyG92IyZNLYk8e6MShHgXDZvNwwPS318nxH4PtibGCturDH6YTM/V5qN/oV8zKTnuQBSsmjk5G8PAyWxScrPbxXbpRgvykg9NkBWUEVV2Ep9OZSERCxfloBKvSEDNxQTWmksXVAK0S0uz7sy+mEVw6M1GCYiMSLxGzXW03Q9dM8YKvLHAEGXmq1Qi1ZIDSxpzXMJLekKHW9E+Aru+yQI9ESwGh7BOLfBkLXlk9EEsxrh4zfFYM0JvKNO/ETd86JeltmJUFJUt8OS35JvYY00YGaMG86w== alice.jones@digital.justice.gov.uk You can find more information about SSH keys in this guidance. 2.3.1.2 Step 2: Register the key with GitHub. The next step is to register your SSH public key with GitHub, using the interface on www.github.com. This guidance assumes you have copied your public key to your computer’s clipboard in the previous step. First, navigate to the GitHub homepage. If you are not logged in already, you will need to log in. Access your settings from the menu that appears when you click on your profile picture in the top right. Once in settings, access your SSH and GPG keys. Here’s a direct link. Click on New SSH key. Paste your key into the dialogue that pops up and click Add SSH key. You can choose any name you like for the ‘title’ of the key. The link with GitHub and the Analytical Platform is now established. You can now return to the analytical platform. Configure your Git name and email address within the analytical platform. To start syncing your work with GitHub, Git needs to know a bit more about you. Within R Studio, access the shell using Tools -&gt; Shell... Then, you need to type the following commands (substitute your user name and email): git config --global user.name &#39;Your Name&#39; git config --global user.email &#39;your@email.com&#39; You’re now ready to start using GitHub! 2.3.2 Using Git with the Analytical Platform More detailed guidance on using Git and GitHub with the Analytical Platform can be found here. 2.4 Guidance on writing code Please read the coding standards. 2.5 Training Resources The data science team maintain a list of R training here. "],
["working-with-big-datasets-and-sharing-them-accessing-data-in-amazon-s3.html", "Part 3 Working with big datasets and sharing them: Accessing data in Amazon S3 3.1 Accessing data in S3 from R 3.2 Importing data from S3 into JupyterLab 3.3 Writing data from JupyterLab to S3 3.4 Uploading data to S3 (from your machine) 3.5 Creating and managing data access groups", " Part 3 Working with big datasets and sharing them: Accessing data in Amazon S3 Amazon S3 is used as the primary storage area for large data files. In contrast to data in your Platform’s home directory, data in S3 can be accessed by multiple Platform users. You will only be able to access S3 buckets (folders) that you have been granted access to. 3.1 Accessing data in S3 from R There are currently two methods of browsing and importing data held in S3 into RStudio. 3.1.1 User Interface We have developed a user interface that allows you to search and browse the files that you have been given access to. You can access this interface by typing the following command into your R Studio console: s3browser::file_explorer_s3() See the documentation for further details. 3.1.2 R Package We have also developed an R Package called s3tools that makes it easier to interact with s3. This enables you to do things like write s3tools::s3_path_to_full_df(&quot;alpha-everyone/iris.csv&quot;) to read directly from S3 into a data frame in R. See the documentation for further details. 3.2 Importing data from S3 into JupyterLab Pandas can import straight from S3: import pandas as pd pd.read_csv(&quot;s3://alpha-everyone/iris.csv&quot;) This requires you to install this library: pip install s3fs Or alternatively use the standard python AWS client boto3. If using Spark (and the Jupyter deployment including the Spark libraries): spark.read.csv(&quot;s3a://blah&quot;) These ‘direct reads’ should be faster than downloading from S3 to your machine and using JupyterLab to uploading to your home drive. 3.3 Writing data from JupyterLab to S3 pandas won’t write files to S3 directly. Instead either: * write data to a temporary file on disk and copy the file to S3 with boto3 * write data to a memory buffer and copy the file to S3 with boto3 like this: from io import StringIO import boto3 csv_buffer = StringIO() df.to_csv(csv_buffer) s3_resource = boto3.resource(&#39;s3&#39;) s3_resource.Object(bucket, &#39;df.csv&#39;).put(Body=csv_buffer.getvalue()) 3.4 Uploading data to S3 (from your machine) 3.4.1 Where data is stored Data in S3 is stored in ‘buckets’, which are conceptually very similar to folders. Each of these ‘buckets’ has a corresponding ‘data access group’ which is the group of platform users who have access to the bucket. Each data access group has one or more administrators, who are able to add and remove people from the group. Typically, a suitable bucket will already exist for the work you are doing, so you just need to ask the data access group administrator to add you. If it does not already exist, you may create a new ‘bucket’. In doing so, you will automatically become the administrator of this data access group. 3.4.2 Uploading data The easiest way to upload data to an S3 buckets is via the AWS GUI. Instructions are as follows: Navigate to the platform control panel, and click on the ‘warehouse data’ tab. Look for the bucket you want to upload data to, and click the ‘Open on AWS’ button, which will take you into the AWS GUI Within AWS, click the blue ‘Upload’ button Drag files in as instructed Click the ‘Upload’ button at the bottom left of the dialogue. You can also move, rename and delete data using the AWS GUI. Select the files by checking the text box, use the More button so see the options. 3.5 Creating and managing data access groups Before creating a new S3 bucket and corresponding data access group, please check that a suitable S3 bucket does not already exist. You can do this by navigating ‘up a level’ within the AWS S3 GUI, by clicking on ‘Amazon S3’ at the top left of the screen. [TODO: Add picture] This will show you a list of all S3 buckets within the platform, including buckets you do not have access to. Assuming a suitable bucket does not already exist, you can create a new one as follows: 3.5.1 Step 1 - Create a new bucket Navigate to the ‘Data Warehouse’ tab of the control panel. At the bottom of the page, click on ‘Create new warehouse data source’. Type a name for your S3 bucket. Note that for technical reasons, all buckets must start with alpha-, which will be populated for you automatically 3.5.2 Step 2 (optional) Grant access to other users, and set their access level Navigate to the ‘Data Warehouse’ tab of the control panel. Click on the name of the bucket you wish to manage. Scroll to the bottom of the page, and start typing a user’s name in the ‘Grant access to this data to other users’ box. If you want to grant administrator priviledges on another user click on the ‘Edit access level’ button next to their name. This will enable them to manage the data access group. Each group can have multiple administrators. "],
["github.html", "Part 4 Using Github with the platform 4.1 Creating your project repo on GitHub 4.2 R Studio 4.3 Jupyter 4.4 Command-line 4.5 Working on a branch. 4.6 Git training Resources 4.7 Safety barriers 4.8 Other tips and tricks [Work in progress!]:", " Part 4 Using Github with the platform Before you can use Github with R Studio or Jupyter, you need to connect them together by creating an ‘ssh key’. Full guidance is here Github enables you to collaborate with colleagues on code and share you work with them. It puts your code in a centralised, searchable place. It enables easier and more robust approaches to quality assurance, and it enables you to version control your work. More information about the benefits of Github can be found here. If you are new to Git and Github, it is worth clarifying the difference between Git and Github. Git is the software that looks after the version control of code, whereas Github is the website on which you publish and share your version controlled code. In practice this means you use Git to track versions of your code, and then submit those changes to Github. This guide provides a step-by-step guide of how to create a GitHub project repo, followed by how to sync with it in R Studio and Jupyter. You can find more in-depth Git training resources here Note: If any of the animated gifs below do not display correctly, try a different web browser e.g. Microsoft Edge, which is installed on your DOM1 machine. 4.1 Creating your project repo on GitHub 4.1.1 Step 1 - Create a new project repo in the moj-analytical-services Github page A GitHub ‘repo’ (short for ‘repository’) is conceptually similar to setting up a project folder on the DOM1 shared drive to save your work, and share it with others. The files in this Github repo represent the definitive version of the project. Everyone who works on the project makes contributions to this definitive version from their personal versions. Note that if you want to contribute to an existing project, you can skip this step. In your web browser go to github.com and make sure you’re signed in. Once signed in, go to the MoJ Analytical Services homepage at https://github.com/moj-analytical-services/ Then follow the steps in this gif to create a new repository. Notes: Make sure that the repository is set to ‘private’. This is the default setting. If you change this setting to ‘public’, your code will be available on the open internet. Make sure the owner is set to ‘moj-analytical-services’. This is the default setting, so long as you have clicked on ‘New’ from the https://github.com/moj-analytical-services homepage. 4.1.2 Step 2: Navigate to your new Repository on GitHub to decide who can see your code Try to be as open as possible about who can view your code. Go to the Settings section of your repository (top right of the repository’s homepage) and then click on Collaborators &amp; Teams on the left hand side panel. From there you can then decide on one of the four options below. They start with the most private all the way to completely public code: PRIVATE: Leave the default setting of your repository so it’s only visible to you as the creator. YOUR TEAM: Can the code be shared within your team? If so, add your team to the repository. ALL PLATFORM USERS: Can the code be shared with all Analytical Platform users? If so, add the ‘everyone’ team to the repository. PUBLIC: Can the code be public? If so, make it a public repository. To do this, click on the ‘Options’ section of the Settings, then scroll down to the ‘Danger Zone’ area that has a ‘Make public’ button. We find that for most of our work, there’s no reason not to add the ‘+everyone’ team of all Analytical Platform users with read access to the code. This is possible as sensitive datasets are not stored in Github. By making code more open (either internally or publicly), users can start to get much more value of out the extremely powerful code search in GitHub. Note you can add one or more teams to a repository, each with different permissions. For example, your team could have write privileges, but the ‘everyone’ team could be read only., 4.2 R Studio Here’s how you can sync with your new GitHub repo in R Studio. 4.2.1 Step 1: Navigate to your platform R Studio and make a copy of the Github project in your R Studio In this step, we create a copy of the definitive GitHub project in your personal R Studio workspace. This means you have a version of the project which you can work on and change. Follow the steps in this gif: Notes: When you copy the link to the repo from Github, ensure you use the ssh link, which start git@github.com as opposed to the https one, which start https://github.com/ 4.2.2 Step 2: Edit your files, track them using Git, and sync (‘push’) changes to Github Edit your files as usual using R Studio. Once you’re happy with your changes, Git enables you to create a ‘commit’. Each git commit creates a snapshot of your personal files on the Platform. You can can always undo changes to your work by reverting back to any of the snapshots. This ‘snapshotting’ ability is why git is a ‘verson control’ system. In the following gif, we demonstrate changing a single file, staging the changes, and committing them. In reality, each commit would typically include changes to a number of different files, rather than the single file shown in the gif. Notes: ‘committing’ does not sync your changes with github.com. It just creates a snapshot of your personal files in your platform disk. Git will only become aware of changes you’ve made after you’ve saved the file as shown in the gif. Unsaved changes are signified when the filename in the code editior tab is red with an asterisk. 4.2.3 Step 3: Sync (‘push’) your work with github.com In R Studio, click the ‘Push’ button (the green up arrow). This will send any change you have committed to the definitive version of the project on Github. You can then navigate to the project on Github in your web browser and you should see the changes. Notes: After pushing, make sure you refresh the GitHub page in your web browser to see changes. That’s it! If you’re working on a personal project, and are not collaborating with others, those three basic steps will allow you to apply version control to your work with Github 4.3 Jupyter There is not the same integration. Use the command-line - see below. 4.4 Command-line Once you are comfortable using the Terminal (in either R Studio or Jupyter) you can do steps 3 and 4 using the following git commands: Select the files that you want to commit: git add &lt;filename1&gt; &lt;filename2&gt; ((This will ‘add’ them to git’s ‘index’ / ‘staging’ area) ‘Commit’ the files you have added: git commit. After calling this command, you need to provide a commit message: in R Studio it provides a popup; in Jupyter it’ll start an editor where you write the message, before saving and exiting it. ‘Push’ your commits to GitHub: git push origin &lt;branch_name&gt;. Most likely your branch name will be master which is the default. So your code would be git push origin master. 4.5 Working on a branch. One of the most useful aspects of git is ‘branching’. This involves a few extra steps, but it enables some really important benefits: Allows you to separate out work in progress from completed work. This means there is always a single ‘latest’ definitive working version of the code, that everyone agrees is the ‘master copy’. Enables you and collaborators to work on the same project and files concurrently, resolving conflicts if you edit the same parts of the same files. Enables you to coordinate work on several new features or bugs at once, keeping track of how the code has changed and why, and whether it’s been quality assured. Creates intutitive, tagged ‘undo points’ which allow you to revert back to previous version of the project e.g. we may wish to revert to the exact code that was tagged ‘model run 2015Q1’. We therefore highly recommend using branches. (Up until now, we’ve been working on a single branch called ‘master’.) 4.5.1 Step 1 (optional): Create an Issue in github that describes the piece of work you’re about to do (the purpose of the branch) Github ‘issues’ are a central place to maintain a ‘to do’ list for a project, and to discuss them with your team. ‘Issues’ can be bug fixes (such as ‘fix divide by zero errors in output tables’), or features (e.g. ‘add a percentage change column to output table’), or anything else you want. By using issues, you can keep track of who is working on what. If you use issues, you automatically preserve a record of why changes were made to code. So you can see when a line of code was last changed, and which issue it related to, and who wrote it. 4.5.2 Step 2: Create a new branch in R Studio and tell Github about its existence Create a branch with a name of your choosing. The branch is essentially a label for the segment of work you’re doing. If you’re working on an issue, it often makes sense to name the branch after the issue. To create a branch, you need to enter the following two commands into the shell: git checkout -b my_branch_name. Substitute my_branch_name for a name of your choosing. This command simultaneously creates the branch and switches to it, so you are immediately working on it. git push -u origin my_branch_name. This tells github.com about the existence of the new branch. 4.5.3 Step 3: Make some changes to address the Github issue, and push (sync) them with Github Make changes to the code, commit them, and push them to Github. 4.5.4 Step 4: View changes on Github and create pull request You can now view the changes in Github. Github recognises that you’ve synced some code on a branch, and asks you whether you want to merge these changes onto the main ‘master’ branch. You merge the changes using something called a ‘pull request’. A ‘pull request’ is a set of suggested changes to your project. You can merge these changes in yourself, or you can ask another collaborator to review the changes. One way of using this process is for quality assurance. For instance, a team may agree that each pull request must be reviewed by a second team member before it is merged. The code on the main ‘master’ branch is then considered to be quality assured at all times. Pull requests also allow you and others working on the project to leave comments and feedback about the code. You can also leave comments that reference issues on the issue log (by writing # followed by the issue number). For example you might comment saying “This pull request now fixes issue #102 and completes task #103”. 4.5.5 Step 5: Sync the changes you made on github.com with your local platform When you merged the pull request, you made changes to your files on Github. Your personal version of the project in your R Studio hasn’t changed, and is unaware of these changes. The final step is therefore to switch back to the ‘master’ branch in R Studio, and ‘Pull’ the code. ‘Pulling’ makes R Studio check for changes on Github, and update your local files to incorporate any changes. 4.6 Git training Resources If you are new to git and you want to learn more, we recommend that you complete the basic tutorial available here. The slides from from the ASD git training are available here (dom1 access only) Using Github with R Introductory interactive tutorial. Quickstart guide and cheatsheet here and in pdf format here. More in depth materials: Learn Git branching Git from the inside out 4.7 Safety barriers The platform has configured simple “safety barriers” to reduce risk of accidentally exposing sensitive data on GitHub. For example it stops you committing a CSV file, because in most circumstances you should not put data into GitHub - it should be kept in an S3 bucket where it can be shared with authorized people. These rules can be overridden if that makes more sense. What How it’s configured Reasoning How to override Data files (.csv, .xls etc) &amp; zip files ~/.gitignore You should not put data into GitHub - it should be kept in an S3 bucket where it can be shared with authorized people. When you add the file: git add -f &lt;filename&gt; Zip files ~/.gitignore It’s better to unpack these files and commit the raw source. You can’t keep track of diffs of individual files if you keep them bundled up. There might be a data file lurking in the zip, which isn’t checked if it is bundled like this. Note: git has its own built in compression methods. When you add the file: git add -f &lt;filename&gt; Large files (&gt;5 Mb) ~/.git-templates/hooks/pre-commit Likely to be data When you commit: git commit --no-verify Pushing to non-official GitHub organizations ~/.git-templates/hooks/pre-push It would be outside MoJ control - not normally allowed. When you push: git push -f &lt;remote&gt; &lt;branch&gt; 4.8 Other tips and tricks [Work in progress!]: 4.8.1 Search the code in MoJ Analytical Services to see who else has used a package. An example of a code search is here. Further guidance is here 4.8.2 Hyperlink to a specific line of code in your project See here for how to do this. 4.8.3 View who made changes to code, when and why using Git blame. See here. An example of this is here. 4.8.4 Make your project available to people on different teams 4.8.5 Assign a reviewer to a pull request, and leave comments. 4.8.6 View how files have changed on the platform and on "],
["deploying-an-r-shiny-app.html", "Part 5 Deploying an R Shiny app 5.1 Basic deployment 5.2 Advanced deployment 5.3 Troubleshooting", " Part 5 Deploying an R Shiny app 5.1 Basic deployment 5.1.1 Summary To create and deploy a Shiny app, you should follow these steps: Copy the app template Create a new webapp Clone the repository Develop the app Manage dependencies Set access permissions Create a release in GitHub Deploy in Concourse Add users to the app Access the app 5.1.2 Copy the app template Go to github.com/new/import Fill in the form: Your old repository’s clone URL: https://github.com/moj-analytical-services/rshiny-template Owner: moj-analytical-services Name: The name of your app, e.g., my-app Privacy: Private Click ‘Begin import’ This copies the entire contents of the app template to a new repository. 5.1.3 Create a new webapp Standard users are not able to create new webapps or webapp data sources themselves. To create a new webapp or webapp data source, ask the Analytical Platform team on the #ap_admin_request Slack channel or by email, if you are a Quantum user. You should provide the URL of the app’s GitHub repository as well as any existing webapp data sources it should be connected to. 5.1.4 Clone the repository Navigate to the app’s repository on GitHub. Click ‘Clone or download’. Ensure that the dialogue says ‘Clone with SSH’. If the dialogue says ‘Clone with HTTPS’ select ‘Use SSH’. Copy the SSH URL. This should start with git@. In RStudio, select ‘File’ &gt; ‘New project…’ &gt; ‘Version control’ &gt; ‘Git’. Paste the SSH URL in the ‘Repository URL’ field. Select ‘Create Project’. 5.1.5 Develop the app Develop the app in RStudio. Your app can take one of several forms: A directory containing server.R, plus, either ui.R or a www directory that contains the file index.html. A directory containing app.R. An .R file containing an R Shiny application, ending with an expression that produces an R Shiny app object. A list with ui and server components. an R Shiny app object created by shinyApp. By default, the template contains server.R and ui.R files, however, you may wish to take a different approach depending on your requirements. For example, using app.R, it is possible to deploy R Shiny apps from within a package, as here. 5.1.6 Manage dependencies Most apps will have dependencies on various third-party packages (e.g., dplyr). These packages change through time and may not always be backwards-compatible. To avoid compatibility issues and ensure reproducible outputs, it is necessary to use a package management system, such as packrat or conda. If using packrat, ensure that it is enabled for your project in RStudio. To enable packrat, select ‘Tools’ &gt; ‘Project Options…’ &gt; ‘Packrat’ &gt; ‘Use packrat with this project’. When packrat is enabled, run packrat::snapshot() to generate a list of packages used in the project, their sources and their current versions. You may also wish to run packrat::clean() to remove unused packages from the list. The list is stored in a file called packrat/packrat.lock. You must ensure that you have committed this file to GitHub before deploying your app. 5.1.7 Set access permissions You can set some access permissions for your app in the deploy.json file that is included with the app template. This file is used by Concourse to detect apps that are ready to build and deploy. The allowed_ip_ranges parameter controls where your app can be accessed from. It can take any combination of the following values [&quot;DOM1&quot;, &quot;QUANTUM&quot;, &quot;102PF Wifi&quot;] or [&quot;Any&quot;]. The disable_authentication parameter controls whether sign-in (using a link or one-time passcode sent to an authorised email address) is required for users to access the app. It can take the values true or false. In general, this should be set to false. When disable_authentication is set to true, users do not need to go through a sign-in process but can still only access an app using a system specified in allowed_ip_ranges. This is a relatively weak security measure, as discussed here. As such, if you wish to disable authentication, you should first discuss this with the Analytical Platform team. 5.1.8 Create a release in GitHub When you’re ready to share your app, you should create a release in GitHub. When you create a release, this is detected by Concourse, which will automatically begin the build/deploy process for your app. Each time you create a new release, Concourse will create a new build. To create a release in GitHub, navigate to the app’s repository. Select ‘release’ &gt; ‘Draft a new release’. Choose a tag version for the release. GitHub provides tagging suggestions at the right of the screen that we advise you to follow. Choose a title for the release. Describe the contents of the release. Select ‘Publish release’. 5.1.9 Deploy in Concourse Once you have created a release in GitHub, Concourse should automatically start to deploy your app within a few minutes. If your app does not deploy automatically, you should first check that the pipeline is not paused. If the app still does not deploy automatically, you can manually trigger a build by pressing the + icon in the top right corner of Concourse. For more information about using Concourse, see Section 8. 5.1.10 Add users to the app If disable_authentication is set to false in the deploy.json file, access to the app will be controlled by email address. You can ask the Analytical Platform team to add or remove users to the access list on the #ap_admin_request Slack channel or by email, if you are a Quantum user. 5.1.11 Access the app Your deployed app can be accessed at repository-name.apps.alpha.mojanalytics.xyz, where repository-name is the name of the relevant GitHub repository. If the repository name contains underscores, these will be converted to dashes in the app URL. For example, an app with a repository called repository_name would have the URL repository-name.apps.alpha.mojanalytics.xyz. When accessing an app, you can choose whether to sign in using an email link (default) or a one-time passcode. To sign in with a one-time passcode, add /login?method=code to the end of the app’s URL e.g. https://kpi-s3-proxy.apps.alpha.mojanalytics.xyz/login?method=code. (This requires the app to have been deployed since the auth-proxy release on 30/1/19.) 5.2 Advanced deployment 5.2.1 Editing the Dockerfile A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. In most cases, you will not need to change the Dockerfile when deploying your app. If your app uses packages that have additional system dependencies, you will need to add these in the Dockerfile. If you are unsure how to do this, contact the Analytical Platform team. A Dockerfile reference can be found here. 5.3 Troubleshooting 5.3.1 Common errors Sometimes, an R Shiny app can deploy successfully but result in the following error: An error has occurred The application failed to start The application exited during initialization This is generic error that means there is an error in your R code or there are missing packages. To try to fix this you should: explicitly reference all third-party packages using the double colon operator (i.e. use shiny::hr() as opposed to hr()); ensure that you have called packrat::snapshot() and committed packrat.lock to GitHub, if using packrat. In general, it is also good practice to: minimise the number of packages you use in your project; test your app early and often; and test using a cloned copy of the app’s repository to avoid issues arising as a result of uncomitted local changes. 5.3.2 App sign-in Some anti-virus software and spam filters pre-click links in emails, meaning that app sign-in links do work. In this case, you should sign in using a one-time passcode, as described in Section 5.1.11. 5.3.3 Packrat If you are having issues with packrat.lock, follow these steps: Delete the entire packrat directory. Comment out all code in the project. Enable packrat using packrat::init(). Capture all package dependencies using packrat::snapshot(). Uncomment all code in the project and install package dependencies one by one. Rerun packrat::snapshot(). Redeploy the app. 5.3.4 Kibana All logs from deployed apps can be viewed in Kibana. To view all app logs: select ‘Discover’ from the left sidebar select ‘Open’ from the menu bar select ‘Application logs (alpha)’ from the saved searches To view the logs for a specific app: select ‘Add a filter’ select ‘app_name’ as the field select ‘is’ as the operator insert the app name followed by ‘-webapp’ as the value click ‘Save’ Log messages are displayed in the ‘message’ column. By default, Kibana only shows logs for the last 15 minutes. If no logs are available for that time range, you will receive the warning ‘No results match your search criteria’. To change the time range, select the clock icon in the menu bar. There are several presets or you can define a custom time range. Kibana also has experimental autocomplete and simple syntax tools that you can use to build custom searches. To enable these features, select ‘Options’ from within the search bar, then toggle ‘Turn on query features’. 5.3.5 Deploying locally If you have a MacBook, you can use Docker locally to test and troubleshoot your R Shiny app. You can download Docker Desktop for Mac here. To build and run your R Shiny app locally, follow these steps: Clone your app’s repository to a new folder on your MacBook. This guarantees that the app will be built using the same code as on the Analytical Platform. Open a terminal session and navigate to the directory containing the Dockerfile. Build the Docker image using: docker build . -t &lt;image tag&gt; where image tag is a tag you want to give the image. Run a Docker container created from the Docker image using: docker run -p 80:80 &lt;image tag&gt; Go to 127.0.0.1:80 to view the app. If the app does not work, follow these steps: Start a bash session in a Docker container created from the Docker image using: docker run -it -p 80:80 &lt;image tag&gt; bash Install the nano text editor using: apt-get update apt-get install nano Open shiny-server.conf using: nano /etc/shiny-server/shiny-server.conf Add the following lines at the beginning of shiny-server.conf: access_log /var/log/shiny-server/access.log tiny; preserve_logs true; Write the changes by pressing Ctrl+O. Exit the nano text editor by pressing Ctrl+X. Increase the verbosity of logging and start the Shiny server using: export SHINY_LOG_LEVEL=TRACE /bin/shiny-server.sh Open a new terminal session. Start a new bash session in the Docker container using: docker exec -it &lt;CONTAINER ID&gt; bash You can find the CONTAINER ID by running docker ps. View the logs using: cat /var/log/shiny-server/access.log For further details, see the Shiny server documentation. "],
["deploying-a-static-web-app.html", "Part 6 Deploying a Static Web App 6.1 Step-by-step guide to depolying an static web app 6.2 Accessing the app 6.3 Advanced deployment", " Part 6 Deploying a Static Web App The following steps to deploy a static HTML web site are as follows: Copy the template project within Github to a new repository, with a name of your choice. Work on your static website - the exposed content will be in the www/ directory and www/index.html will be the landing page. When you’re ready to share it, access the services control panel, find your app, and click ‘Build now’. This will prepare your site for deployment. Set the desired access permissions in deploy.json (i.e. whether it should be available for DOM1, Quantum, or external). When you’re ready to share it, in GitHub create a ‘release’ and it will deploy in a few minutes. Ask the Platform team to add and remove users from your app in the #ap_admin_request Slack channel. Step-by-step instructions are below. 6.1 Step-by-step guide to depolying an static web app 6.1.1 Copy the template project into a new Github repository Begin by making a copy of the template project on Github: https://github.com/new/import Enter https://github.com/moj-analytical-services/webapp-template in the input box entitled ‘your old repository’s clone URL:’ Ensure the ‘owner’ of the new repository is ‘moj-analytical-services’ and choose a name for your repository: Make sure the repo is ‘private’ (this should be the default value): Click ‘Begin import’ 6.1.2 In your chosen development enviroment, clone the git repository You can find the clone link on the Github repository. To download a copy to start editing on your local machine, you need to ‘clone’ the repositry. If you’re using a shell: git clone git@github.com:moj-analytical-services/YOUR-REPO-NAME.git 6.1.2.1 Further notes if you’re having trouble finding your new repo’s url If you navigate to your new repository’s home page (which will have a url in the form https://github.com/moj-analytical-services/your_name_goes_here), you can use the following buttons to access this url (make sure you click the ‘ssh’ button): 6.1.3 Work on your web app Work on your web app using your chosen development enviroment. As you work, commit your changes to Github using your chosen Github workflow. 6.1.4 Scan organisation and deploy Include a deploy.json file so that Concourse will automatically build and deploy your app, so that it is running and customers can access it. For more about this see: Build and deploy guidance 6.1.5 Grant secure access to the app To grant access to someone, in the Control Panel’s Wepapps tag find your App and click “Manage App”. In the ‘App customers’ section you can let people view your app by putting one or more email addresses in the text box and clicking “Add customer”. You can also let anyone access it by setting &quot;disable_authentication&quot;: true in the deploy.json and redeploying it - see above. Note that users can only access the app from a computer on a specified corporate network. These are defined in the deploy.json under the parameter allowed_ip_ranges - see below. 6.1.5.1 deploy.json parameter notes allowed_ip_ranges e.g. [&quot;DOM1&quot;, &quot;QUANTUM&quot;, &quot;102PF Wifi&quot;] or [&quot;Any&quot;] disable_authentication To let anyone access the app, set this to true, otherwise false restricts it to people authorized for the app in the Control Panel If you deployed with authentication enabled users are granted access to the app using a list of email addresses separated with a space, comma or semicolon. Changes to deploy.json only take effect when committed to GitHub, a Release is created and the deploy is successful (see Scan organisation and deploy) 6.2 Accessing the app Depending on the disable_authentication setting, the website will either be available directly or authenticated via email. The URL for the app will be the respository-name followed by apps.alpha.mojanalytics.xyz. So for the example project above “static-web-deploy”, the deployment URL will be https://static-web-deploy.apps.alpha.mojanalytics.xyz. Note that characters that are not compatible with website URLs are converted. So, repositories with underscores in their name (e.g. repository_name.apps...) will be converted to dashes for the URL (e.g. repository_name.apps...). 6.3 Advanced deployment This section contains guidance for advanced users on app deployment. 6.3.1 Can I change my build? Yes - if you know Docker, you are welcome to change the Dockerfile. "],
["data-pipelines.html", "Part 7 Data pipelines 7.1 Summary 7.2 Concepts 7.3 Set up a pipeline", " Part 7 Data pipelines 7.1 Summary You can deploy your data processing code to the cloud. Airflow is a tool on the Analytical Platform that is a managed place for your “data pipeline” to run. This can be useful, for example, to: run a long job overnight run it on a regular schedule (e.g. every night) when multiple outputs require the same intermediate result, it can be calculated once for them all people other than yourself can see the run and its output (compared to if you run it in your R Studio or Jupyter) you keep a history of all the pipeline runs, showing what you ran, the logs, what tasks failed and how long it took 7.2 Concepts A data pipeline is referred to in Airflow as a “DAG”. It is made up of “tasks” which are arranged into a Directed Acyclic Graph. A DAG is a Directed Acyclic Graph. This could be simple: Task 1 -&gt; Task 2 -&gt; Task 3 (meaning run Task 1 then Task 2 then Task 3). Or a task can be dependent on multiple previous tasks, and when its complete it can trigger multiple tasks. The “Acyclic” bit just means you can’t have loops. Each task is a GitHub repository, containing code files that will be run (e.g. R or python) plus bits that define the environment it runs in - a Dockerfile and an AWS IAM policy. You define the DAG to run on a regular schedule, and/or you can run it by clicking the “Play” button on the Airflow web interface 7.3 Set up a pipeline 7.3.1 Task repository Each task should be a git repository: It should be under the https://github.com/moj-analytical-services organization The name of the repo is recommended to start with airflow- The repo should include: Dockerfile - contains commands to set-up a Docker image that does your data processing. Here’s an example python one: # Begin with a standard base image (from https://hub.docker.com/_/python/ ) FROM python:3.7 # Update the system libraries # (The python base image is based on debian) RUN apt-get update # Install new system libraries RUN apt-get install -y --no-install-recommends \\ python-numpy # Set the default place in the image where files will be copied WORKDIR /usr/src/app # Copy files from the repo into the image COPY requirements.txt ./ COPY . . # Install python dependencies RUN pip install --no-cache-dir -r requirements.txt # Run the data processing # This is the equivalent of typing: python ./my-script.py CMD [ &quot;python&quot;, &quot;./my-script.py&quot; ] Tip: You can put several related scripts in one repo, and choose which one gets run using an environment variable e.g. https://github.com/moj-analytical-services/airflow-magistrates-data-engineering/blob/88986cf7dbcca9b7ebc21bc2d1286464615739d7/Dockerfile#L19 Tip: Some python packages, such as ‘numpy’ or ‘lxml’, come with C extensions which can cause bother. If installed via pip (requirements.txt) those C extensions are compiled, which can be slow and liable to failure. The Dockerfile example above shows how to avoid these problems with numpy (which is needed by pandas) - it is installed instead using a debian package: python-numpy. deploy.json - This is for Concourse to build the repo into a Docker image For example: { &quot;mojanalytics-deploy&quot;: &quot;v1.0.0&quot;, &quot;type&quot;: &quot;airflow_dag&quot;, &quot;role_name&quot; : &quot;airflow_enforcement_data_processing&quot; } The version number does nothing. The role_name needs to be: unique to the Analytical Platform prefixed airflow_, otherwise you’ll get a build error like this: An error occurred (AccessDenied) when calling the GetRole operation: User: arn:aws:iam::593291632749:user/dev-concourse-role-putter is not authorized to perform: iam:GetRole on resource: role airflow-prison-population-hub iam_policy.json - This tells Concourse to create an AWS role that gives your code permission to access AWS resource, such as data in S3 buckets. e.g. See: https://github.com/moj-analytical-services/airflow-enforcement-data-engineering/blob/master/iam_policy.json for listing, reading and writing to S3 buckets. 7.3.2 Test your image - locally on a Macbook If you have a Macbook you can use it to test that your Docker image will build, which is much quicker to debug than waiting for Concourse every time you make a change. This can be useful for checking your Dockerfile syntax is right and the dependencies install ok. However when you run the Docker image, it won’t be able to access your project data (from S3), because your machine doesn’t have your AP account’s AWS credentials. This is right because you shouldn’t have sensitive data on your Macbook in general. So the Docker image will fail when it tries to access AWS data, but you’ve still usefully checked the build works and the install of dependencies. You need to have installed Docker for Mac. When it is running you can see its whale icon in the task bar. It consumes 2GB RAM, so you may well not want it to run everytime your machine starts up - change this under “Preferences…” then uncheck “Start Docker when you log in”. To test your Docker image: Clone your repo to your Macbook. e.g. in a terminal: git clone git@github.com:moj-analytical-services/airflow-occupeye-scraper.git This authenticates GitHub using SSH, so if this is the first time on your machine, you’ll need to create an SSH key and add it to your GitHub account first. Build the repository (from the directory with the Dockefile in it): cd airflow-occupeye-scraper docker build . -t my-pipeline:0.1 (If you get Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? you need to start ‘Docker’ application from your Mac’s Launchpad) This goes through each line (“step”) of the Dockerfile and builds (or executes) each in turn (apart from the final CMD). The first build can take a while, but the result of each step is saved to disk, so if you change a step and do another build, it only needs to restart from the step that is changed. You might want to run the image as a container, which runs your data processing code specified in the CMD of the Dockerfile: docker run my-pipeline:0.1 Which should hopefully get as far as talking to AWS before failing. You can ‘shell into’ the linux container to take a look for debug purposes: docker run -it my-pipeline:0.1 bash And if bash is not available, try sh. 7.3.3 Build the image Concourse will: build your GitHub repository into a Docker image upload the Docker image to a private location in AWS Elastic Container Registry creates the AWS Role For more about the build see: Build and deploy guidance 7.3.4 Airflow DAG You need to define a DAG of your Tasks, so that Airflow can run it. A DAG is defined in its own .py file in the airflow-dags repository. Normally you would clone the repo, add the new DAG file, commit it to a branch and create the Pull Request. You can achieve this in R Studio, Jupyter or on your local machine if you have git, and then use GitHub to create the Pull Request. Alternatively you could simply click the “Create new file” button in GitHub and use the web interface to edit it and create a pull request in one go! Use an existing DAG as an example: https://github.com/moj-analytical-services/airflow-dags/blob/master/mags_curated.py The image is the address of the Docker image - look in the Concourse build log: successfully tagged ... Tip: Before you commit, run the file with python to check the python syntax is ok. If it gets as far as giving you import errors then you know the python syntax is ok. (If you can be bothered to install the python libraries you could run it all) Merging the Pull Request needs to be done by the ASD Data Engineering team, to ensure the IAM permissions are acceptable. The DAG will appear in the Airflow web interface a couple of minutes after the Pull Request is merged. 7.3.5 Airflow web interface The Airflow web interface allows you to see the data pipelines running, view the logs, see past runs, etc. Link: https://airflow.tools.alpha.mojanalytics.xyz You can manually start (“trigger”) the pipeline with this button: "],
["build-and-deploy.html", "Part 8 Build and Deploy 8.1 deploy.json 8.2 Starting a build/deploy", " Part 8 Build and Deploy Concourse is the Analytical Platform service that automatically builds and deploys R Shiny apps, Web apps and Airflow data pipelines. This is also known as ‘continuous integration’ and ‘continuous deployment’ (CI/CD). 8.1 deploy.json Concourse automatically scans git repositories in the moj-analytical-services Github organisation to find repos that are ready to build/deploy. It does this by checking whether repositories contains a ‘magic’ file (on the master branch) that controls the build/deploy: deploy.json. The scan happens every 24 hours, or sooner if a repo in moj-analytical-services is tagged with a release on GitHub (or if it is urgent it can be triggered by an admin). For the format of deploy.json, see the relevant subject: R Shiny app Web app Airflow pipeline Changes to deploy.json only take effect when committed to GitHub (master branch), a Release is created and the deploy is successful (see Scan organisation and deploy). 8.2 Starting a build/deploy To trigger a Concourse build/deploy, make sure your code is committed and pushed to GitHub and then create a release on GitHub. Call the first release v0.0.1, then v0.0.2 etc. The version should always go up in number, or it won’t build. It’s good practise to use semantic versioning. Your task appears in Concourse within 30 seconds. Find it in Concourse to start it as follows: Open Concourse: https://concourse.services.alpha.mojanalytics.xyz/ Select team ‘main’. Click ‘login with ap-main’ followed by your GitHub login and possibly 2FA for GitHub and possibly 2FA for AP. Click the hamburger icon: (top-left) to see the list of repos that can be built. Click your repo name to see its diagram with the big ‘Deploy’ box in the middle, inputs and outputs. Click on the ‘deploy’ box to see details of its builds. You can see previous builds (numbered - e.g. “8 7 6 5 4 3 2 1”). The colour of the build number box gives the status: Grey = paused or pending Yellow = in progress Blue = paused Green = built ok Red = build failed Brown = aborted Orange = errored To start the job (this first time): Click play (on the left, against your repo name) AND the “+”: It should go grey (pending) After 30 seconds it should got yellow (building). If it does not go yellow, check the task is not paused (and possibly the job itself is paused) - press play if is showing. It will build/deploy, taking 5 minutes to an hour. You can track progress and see any errors like this: Subsequent build/deploys are started about 5 seconds after doing a GitHub release (unless someone pressed the pause button on the Concourse task). You have to refresh Concourse in your browser to see a fresh build. "],
["user-administration.html", "Part 9 User Administration 9.1 Teams and data access groups", " Part 9 User Administration 9.1 Teams and data access groups We updated the control panel on 4th May 2018. You can now manage data access groups directly in the control panel. Github teams no long have any effect on data access permissions. We are working on updating the documentation, which will appear here when complete. "],
["acceptable-use-policy.html", "Part 10 Acceptable use policy 10.1 Scope 10.2 Who this policy applies to 10.3 General principles 10.4 GitHub", " Part 10 Acceptable use policy 10.1 Scope This acceptable use policy covers the use of the Analytical Platform and all associated software and applications. This policy applies in addition to the MoJ acceptable use policy. 10.2 Who this policy applies to This policy applies to all users of the Analytical Platform. 10.3 General principles All users will: report any security incidents, including a loss of data, in line with the relevant MoJ, HMPPS or HMCTS procedures; report any breach of this acceptable use policy to the Analytical Platform team; follow all relevant information governance procedures; protect their login credentials appropriately; create secure passwords following best practice guidelines (see here); ensure that two-factor authentication is enabled when accessing GitHub and the Analytical Platform; sign out of the Analytical Platform when access is not required; understand they and MoJ have a legal responsibility to protect personal and sensitive information; understand that their use of the Analytical Platform may be monitored; ensure that all transfers of data onto and within the Analytical Platform are conducted safely and securely; not access the Analytical Platform from any non-MoJ IT system, such as a personal computer; not share their account or login credentials with any other person; not use the same login credentials for more than one system or purpose; not store any data on the Analytical Platform that is classified as SECRET or TOP SECRET; not move any data to the Analytical Platform without completing a data movement form; not attempt to access any data, apps or software on the Analytical Platform without the appropriate permission; and not use the Analytical Platform to undertake any illegal activity or any activity that could harm MoJ’s reputation or compromise the security of data or IT systems. App admins and data source admins will: ensure that app and data source users have the correct read/write permissions; ensure that app and data source users only have access to the minimum data required for them to perform their job; and regularly review access permissions for app and data source users, including when users join or leave MoJ, or move within MoJ. 10.4 GitHub In almost all cases, work must be stored in private repositories in the MoJ Analytical Services organisation. You may only store work in a public repository if you have: verified that the work contains no sensitive information or secrets; obtained prior written permission from your line manager; and followed the guidance on making source code open and reusable. GitHub may be used to store: source code; reports and documentation; and small, non-sensitive data sets (on a temporary basis when alternatives such as S3 are not practical) in accordance with the following restrictions. All users will: not store any large data sets (&gt; 1,000 records) in GitHub; not store any data, source code or documentation containing sensitive information in GitHub; not store any data, source code or documentation containing personal information in GitHub; not store any credentials or secrets, such as usernames, passwords, database connection strings or API keys in GitHub; provide access to private repositories on a need-to-know basis; store all MoJ work in the MoJ Analytical Services organisation; not store any work in public repositories without obtaining prior written permission from their line manager; verify that any work stored in public repositories does not contain any sensitive or personal information; and follow the guidance on making source code open and reusable. "],
["information-governance.html", "Part 11 Information governance 11.1 Data management 11.2 Data movements 11.3 Data Offshoring 11.4 Reporting security incidents", " Part 11 Information governance 11.1 Data management 11.1.1 Permissions and access Access to data and apps on the Analytical Platform should be provided on a need-to-know basis. If you are an admin for data source, you can control which users have access from the Analytical Platform control panel. Some app access permissions can be specified in deploy.json (see Section ??), however, users cannot manage an app’s user access list themselves. DOM1 and MacBook users can request access to a data source (for themselves, other users or customers) on the #ap-admin-request Slack channel. Quantum users should email the Analytical Platform team. If requesting access to a data source, you should provide the GitHub usernames of the relevant users. If requesting access to an app, you should provide the email addresses of the relevant users. 11.1.2 Data retention 11.1.2.1 Amazon S3 Files stored in Amazon S3 are retained indefinitely until they are deleted. Files stored in Amazon S3 are backed up automatically. Once files are deleted from Amazon S3, they are deleted permanently along with all backups and cannot be restored unless the bucket is versioned. Versioning is disabled by default. 11.1.2.2 Home directory Files stored in users’ home directories are retained indefinitely until they are deleted. Files stored in users’ home directories are backed up to Amazon S3 automatically. Previous versions of files stored in users’ home directories are also backed up to Amazon S3. Once files are deleted from a user’s home directory, the backup is retained for a further 90 days and can be restored. To request that a file be restored or to access a previous version of a file, contact the Analytical Platform team. 11.1.2.3 Best practice guidelines All users should comply with the following best practice guidelines for retaining personal data. All users will: be aware when they are working with personal data; consider and justify why personal data needs to be retained and for how long; identify when personal data needs to be kept for public interest archiving, scientific or historical research, or statistical purposes; review the need to retain personal data on a regular basis; erase or anonymise personal data when it is no longer required; and erase data as required in accordance with an individual’s ‘right to be forgotten’ under the Data Protection Act 2018. The relevant information asset owner (IAO) should be able to advise if any additional data retention policies or requirements apply. 11.2 Data movements All data movements should take place safely and securely to ensure that data is protected at all times, including when in transit. To facilitate this, when moving any data onto the Analytical Platform, you must complete a data movement form. The data movement form can be found here. Guidance on completing the form can be found in Section 11.2.1. Depending on the details of the data movement, you may also be required to complete or update: A technical migration form; Data protection impact assessments (DPIAs); Privacy impact assessments (PIAs); Privacy notices; or Information management and asset logs. Before completing a data movement form, you should ensure that you have already obtained all necessary approvals and completed all required supplementary documentation. 11.2.1 Data movement form guidance You may not be required to complete all of the questions below depending on the details of your data movement. 11.2.1.1 Contact information Name Email address Phone number Is the main person carrying out the data movement the same as the requestor? Name of the main person carrying out the data movement Email address of the main person carrying out the data movement Phone number of the main person carrying out the data movement 11.2.1.2 The data What data is being moved? What fields does the data contain? What is the security marking of the data? The Analytical Platform is suitable for data classified as OFFICIAL and OFFICIAL-SENSITIVE. SECRET and TOP SECRET data is not allowed on the Analytical Platform. Information on security markings and classifying information can be found here. Will the data be modified or changed in order to mask sensitive content or personal information? Guidance on masking and anonymisation can be found in sections 14.4.1 and 14.4.2. How will the data be changed or modified in order to mask sensitive content or personal information? Will the data be modified or changed before or after being moved to the Analytical Platform? 11.2.1.3 Personal data Does the data include personal data? Personal data is information that can be used to identify a person directly or indirectly in combination with other information. Personal data includes names, identification numbers, addresses and online identifiers. Guidance from the Information Commissioner’s Office (ICO) on identifying personal data can be found here. Further information on handling personal data in MoJ can be found here. 11.2.1.4 Data controller Is MoJ the data controller responsible for the data? A data controller is an entity registered with the Information Commissioner’s Office (ICO) that exercises overall control over the purposes and means of the processing of personal data. MoJ is the controller for MoJ HQ, HMPPS, HMCTS, LAA, OPG and some other agencies and public bodies. The Analytical Platform guidance contains a list of agencies and arm’s length bodies that are data controllers in their own right. The relevant information asset owner (IAO) should also be able to advise who the correct data controller is. Who is the data controller responsible for the data? The data controller could be another MoJ agency or public body (for which MoJ is not the responsible controller), another government department or a third party. You can use the ICO Data Protection Register to determine whether an entity is a controller. The following agencies and public bodies are data controllers: Criminal Injuries Compensation Authority (CICA) Children and Family Court Advisory and Support Service (CAFCASS) Criminal Cases Review Commission (CCRC) Legal Services Board (LSB) Parole Board for England and Wales Youth Justice Board for England and Wales (YJB) Civil Justice Council (CJC) Family Justice Council (FJC) Sentencing Council for England and Wales Office for Legal Complaints (Legal Ombudsman for England and Wales) The Official Solicitor to the Senior Courts The Public Trustee Prisons and Probation Ombudsman (PPO) Is a suitable data sharing agreement in place with the data controller? Is the controller aware of the data movement and the intended use of the Analytical Platform as a data processor? 11.2.1.5 Data protection and privacy Have the relevant Data Protection Impact Assessments (DPIAs) and Privact Impact Assessments (PIAs) been created or updated and reviewd by the data protection team? If you are unsure whether a DPIA or PIA already exists or needs to be created, you should contact the relevant Information Asset Owner (IAO). Further information on DPIAs and PIAs can be found here. You can also contact the Data Protection Officer mailbox here. Have the relevant privacy notices been updated to reflect use of the Analytical Platform? A privacy notice is required to let customers using a product or service know: who is collecting the data; what data is being collected; if they are required to provide the data; what is the legal basis for processing the data; how the data will be used; if the data will be shared with any third parties; how long the data will be held for; if the data will be used for automated deecision-making, including profiling; what rights they have in relation to their data; and how they can raise a complaint about the handling of their data. Further information on data privacy and privacy notices can be found here. ICO guidance can also be found here. 11.2.1.6 The data movement Why is the data being moved? What is the source of the data? Where will the data be stored on the Analytical Platform? Which S3 bucket will the data be stored in? How many records does the data contain? Who will have access to the data on the Analytical Platform? How will the data be moved? Describe how the data will be moved from the source location. Will the data be moved electronically, by email or by physical media? Will it be uploaded manually to S3 or a home directory or will there be a direct integration with another system? Is a technical migration form required? A technical migration form is not required for one-off data movements of static files (such as .zip files and .csv files) that are carried out manually (i.e. by email or direct upload to S3 or a home directory). A technical migration form is required for all other data movements. If you are unsure whether your data movement requires a technical migration form, please contact the Analytical Platform team. A template technical migration form can be found here. To access this template, you must be signed in to Office 365. If you are unable to access the template, please contact the Analytical Platform team. Technical migration form Please send your form as an attachment to the Analytical Platform team. Is this a one-off or recurring data movement? When is the data movement expected to occur? What is the intended frequency of the data movement? When is the first data movement expected to occur? Will the data movement be in place for a fixed period of time? When will the final data movement occur? When will the data movement request be reviewed? Recurring data movements that occur indefinitely should be reviewed at least every two years. 11.2.1.7 Data retention How long will the data be retained on the Analytical Platform? Is this in line with departmental and Analytical Platform data retention policies? Guidance on data retention can be found in Section 11.1.2. 11.2.1.8 Information Asset Owner (IAO) All data in MoJ is assigned an Information Asset Owner (IAO). The IAO is responsible for the registration, labelling, storage, transfer and retention of that data. You should seek approval from the relevant IAO for all data movements. Does the data movement require approval by an Information Asset Owner (IAO)? IAO name IAO email address Has the IAO approved this data movement? 11.2.1.9 Senior Information Risk Officer (SIRO) Information on the responsibilities of SIROs can be found here. SIRO approval may be required depending on local information assurance policies. SIRO approval is usually required when a data movement involves non-routine risks. The relevant IAO should be able to advise if SIRO approval is also required. Does the data movement require approval by a Senior Information Risk Officer? SIRO name SIRO email address Has the SIRO approved this data movement? 11.2.1.10 Information assurance Have you discussed the data movement with the relevant information security team(s)? The relevant information security teams are: MoJ HQ – cybersecurity team HMCTS – information assurance team HMPPS – information management and security team Have you considered all other information assurance requirements, including, but not limited to, those arising under the Public Records Act, the Official Secrets Act, the Data Protection Act and the Freedom of Information Act? 11.2.1.11 Confirmation Have you read and understood the Analytical Platform guidance and acceptable use policy? 11.2.2 Data Minimisation It is good practice to undertake data minimisation when moving data to the Analytical Platform as all software imposes memory limits. Data minimisation is also a princicple of GDPR. Any personal data used should be: adequate; relevant; and limited to what is necessary. In practice, you may want to consider whether a subset of a larger data set (removing irrelevant fields or observations) is sufficient for your analysis. 11.3 Data Offshoring The Analytical Platform is hosted on Amazon Web Services (AWS) (a US company part of the Amazon group) with primary data storage and processing locations in their European regions. 11.4 Reporting security incidents As soon as you become aware of an actual or potential security incident, including a loss of data, you should follow the guidance here and here. "],
["how-to-get-support.html", "Part 12 How to get Support 12.1 Summary 12.2 Intro 12.3 Routes of Support 12.4 How to ask for support: Creating a Reproducable Example 12.5 Raising Issues", " Part 12 How to get Support 12.1 Summary The Analytical Platform team in Digitech are responsible for providing access to software like R Studio and Jupyter. Analysts themselves are responsible for the code they write in these tools, and the Platform team is not responsible for assisting with problems with this code. As a rule of thumb, if the problem you’re experiencing would also occur with R Studio or Python installed on a standalone computer, then the platform team don’t offer support. If you’ve read through the user guidance and are still stuck, the best place to go for support is the analytical-platform Slack channel for issues with the platform, or the R channel and python channel to get support from peers in your code. What follows provides further details about how to ask for support, and what you can do to get your issues resolved as quickly as possible. 12.2 Intro When using the analytical platform you may experience problems with administration and the platform infrastructure (such as logging in and authorisation) issues with your code (running R or Python scripts) or the platform not functioning as expected, each of these will have a different support path as set out below. For every problem though there are a few quick steps you can try that will make it easier to provide help and may even fix your issue outright. With the exception of direct administration issues such as resetting authorisation try to: Reproduce the issue: if you can reliably recreate the issue time after time this will make it easier to pin point exactly what is going wrong. This may not always be possible with intermittent faults. Be specific: isolate the problem as much as possible and narrow it down to the smallest segment. It’s much easier and quicker if you can pin point a function or line of code, or the exact step that is causing a failure. Check if it is a known issue: it could be that the problem is experienced regularly by others or is a known bug currently being fixed. The place to go to check this will vary on the problem but checking Github issue logs, googling the problem, checking stackoverflow and browsing the Slack channels can often highlight the best next steps to take. 12.3 Routes of Support 12.3.1 Platform Support The MoJ Digitech team is responsible for supporting problems with the underlying analytical platform. If there is a issue relating to the running of the platform or surrounding infrastructure then they can provide support. Examples of this include: Bugs or unexpected issues Platform crashing or inaccessible Deployment of Shiny apps not working Administration of accounts Reset authorisation Access priviledges not working as expected This does not include any problems which occur as a result of the limitations or incorrect use of software by users on the platform. If issues occur whilst running code or using version control please see the sections below. Remember the first port of call for using the platform should always be this user document. The MoJ Digitech team have a Slack channel where you can post questions and a Github page where you can raise a formal issue ticket. For only those users who cannot access slack or Github (i.e. some non DOM1 users) there is a direct email address which can be used: analytical_platform@digital.justice.gov.uk - but this will be monitored for emails from non DOM1 users. We direct users to Slack for support because that means that users can quickly see whether similar issues are being faced by others, and Slack provides a more efficient way of getting the information we need from users to fix their problems. 12.3.2 Coding Support (R &amp; Python) There is no team who is responsible for offering support, but we have an active community of colleagues who offer support voluntarily. Users are responsible for the code they create and are expected to debug and solve problems themselves. If you need support, there are a multitude of resources: This platform user guidance document contains a number of different sections explaining how the platform interfaces with R and common issues that may occur. There is an R slack channel which is visited frequently by R users from across ASD, please use this forum to ask questions or post examples of code, putting your problem out in the open is also a great way to help others who may be experiencing the same thing. Stack Overflow and Google are powerful tools when you encounter a problem - it’s very unlikely that you are the first to come across it. Stack Overflow also has the added benefit of allowing you to sign up and post questions publicly, massively widening the chance of finding someone who can help. Be aware not to publish data with questions and be sure to follow the guidance of reproducable examples below. An ASD R training group also exists who can help getting you up to speed with R (unfortunately there isn’t yet one for Python), this is led by Aidan Mews who can be contacted to discuss the option of formal R training or guidance on online based tutorials. If you’ve explored the above options and are still having problems, then the Data Science Hub (DaSH) team will try to provide assistance, although please be aware that team members are under no obligation to do so and appreciate the time this may take away from their other work. Data science team members are active contributors to the Slack channels and creating a post there is the preferred method of contact. 12.3.3 Git Support (Version control) Similarly using git and interfacing with Github are processes that users are expected to learn and manage themselves. Git and Github are hugely beneficially to analysts as they provide a single version of the truth, incorporate continual QA and allow multiple users to work consectutively on a single file (just to name a few of the benefits). The set up and configuration of Git is covered in this guidance but ff you have any further queries related to the use of git or Github there are the following routes of support: There is a Git/Github training course being run by the Data Science Hub which will introduce you to the key concepts and provide you a practical example of how it works. It is highly recommended you take this course when first starting out with git. To find out about when the next session is happening, or ask question about git, you can use the data science slack channel. 12.4 How to ask for support: Creating a Reproducable Example When posting a problem or question on a forum (whether this be the ASD slack channel or a public website such as Stack Overflow) it is vital to ensure that other users can reproduce your example, and therefore understand your issue. Posting a question with no example or an example which is too large will prevent otheres from being able to help effectively. When creating a minimal reproducable example: Isolate the section of code which is causing the problem, ensure that the example can be run without the need for any prior processing or code. Remove any real data, helpers cannot be expect to have access to the same dataset so providing a small dummy dataset is important. In R you could use the built in datasets such as mtcars or iris which all R users have access to. Provide the expected output, be clear as to what you are trying to achieve and what the correct outcome will look like. Details on how to produce a good reproducible example can be found here: https://stackoverflow.com/help/mcve https://github.com/tidyverse/reprex 12.5 Raising Issues Rather than posting questions on Slack you could also visit the analytical platform Github page and raise a formal issue ticket. This will then flag your issue directly to the MoJ Digitech team who will respond through the issue log on Github. There is a fixed format for raising issues which should be followed to ensure the team can help: https://github.com/ministryofjustice/analytics-platform/blob/master/.github/ISSUE_TEMPLATE.md This approach of raising issues also applies to any package, model or project that has a Github page. These issue logs are the most common method for highlighting problems in open source project, many R packages for example have a Github page where you can raise issues with the developer or search to see whether your problem has already been raised. "],
["common-errors-and-solutions.html", "Part 13 Common Errors and Solutions 13.1 Failed to lock directory 13.2 rsession-username ERROR session hadabend 13.3 Status Code 502 error message 13.4 Unable to access data using aws.s3 package. 13.5 s3tools::s3_path_to_full_df() fails on Excel file 13.6 Two-factor authentication problems 13.7 I’m having problems deploying a Shiny app", " Part 13 Common Errors and Solutions 13.1 Failed to lock directory This error is typically encountered after a failed package install. Error ERROR: failed to lock directory ‘/home/robinl/R/library’ for modifying Try removing ‘/home/robinl/R/library/00LOCK-readr’ Solution Run the following: install.packages(&#39;pacman&#39;) pacman::p_unlock() If that does not work, or if you have trouble installing the pacman package, try the following: Go to Tools -&gt; Shell and type: rm -rf /home/robinl/R/library/00LOCK-readr See here for more details. Be careful with the rm command! 13.2 rsession-username ERROR session hadabend Errors like the following can typically be ignored: [rsession-aidanmews] ERROR session hadabend; LOGGED FROM: rstudio::core::Error {anonymous}::rInit(const rstudio::r::session::RInitInfo&amp;) /home/ubuntu/rstudio/src/cpp/session/SessionMain.cpp:1934 They seem to occur when R Studio has been unable to restore your session following a crash. Note the items in your R environment will no longer be there following a crash, and you’ll need to re-run your scripts to bring your data back into memory. Crashes often occur when you run out of memory. For now, you can use pryr to track your memory usage - see here. We are working on giving users greater visibility of their memory usage. 13.3 Status Code 502 error message R Studio Server is single-cpu (single thread), which means it can’t ‘do two things at once’. If you ask it to, sometimes one of the operations will timeout. The ‘Status code 502’ message is basically a timeout message. Usually this doesn’t cause anything to crash. You just need to wait for the currently-running code to finish executing and try again. One example of when this can happen is if you attempt to save a file whilst a long-running script is running. R Studio has to wait until the script has finished running to attempt to save the file. However, sometimes the wait is too long, causing a timeout. In this case, you just need to wait for the code to finish running, and then press save again. 13.4 Unable to access data using aws.s3 package. Unfortuntely aws.s3 does not support the granular file access permission model we are using on the platform. Specifically, it is unable to automatically provide the user with the right file access credentials. We provide s3tools as a solution to this problem, which manages your credentials for you. We recommend that, where possible, users should use s3tools. Where this is not possible, include a call to s3tools::get_credentials() prior to making the call to aws.s3, and this will guarantee that fresh credentials are generated before your call to aws.s3 13.5 s3tools::s3_path_to_full_df() fails on Excel file s3tools::s3_path_to_full_df attempts to read in data from various filetypes, including Excel, but this sometimes fails. If it does, you have two options: 13.5.0.1 Option 1: Use s3tools::read_using() This allows you to specify what function you want to use to attempt to read the file. So, for example you can do: s3tools::read_using(openxlsx::readWorkbook, path = &quot;alpha-everyone/my_excel.xlsx&quot;) to attempt to read the file alpha-everyone/my_excel.xlsx using openxlsx::readWorkbook 13.5.0.2 Option 2: Save the file to your project directory and load it from there, rather than from S3 s3tools::get_credentials() aws.s3::save_object(&quot;my_excel.xlsx&quot;, &quot;alpha-everyone&quot;, &quot;file_name_to_save_to_in_home_directory.xlsx&quot;) and then read it in using e.g. openxlsx::readWorkbook(&quot;file_name_to_save_to_in_home_directory.xlsx&quot;) Note, it’s best to avoid using aws.s3 directly, see here 13.6 Two-factor authentication problems Two-factor authentication (2FA) is critical to the security of the platform. We have opted to use smartphone-based authentication apps as hardware tokens, like the RSA device you use to log in to DOM1, are expensive and SMS codes are susceptible to interception. Note that there are two layers of 2FA in action on the platform: Your GitHub account must have 2FA enabled. When you log in to GitHub, your session will stay active for a month before you need to re-enter your 2FA code. Your GitHub username identifies you to the platform, and we use this identity to control access to data and other resources once you’ve logged into the platform. You therefore must be logged into GitHub to use the platform. Your Analytical Platform account has a separate 2FA step. You will be prompted to set this up the first time you access the platform and on subsequent uses, depending on the machine you use and the network it’s connected to: From a corporate-networked machine (DOM1 or QUANTUM) or corporate wifi (e.g. MoJDigital) then you will not be challenged for this. Otherwise (e.g. working from home on a non-corporate-networked machine) you’ll be challenged to provide the 2FA code during every sign-in to access each part the platform: Control Panel, R Studio, Grafana etc. This security step lets you log into the platform and use it. Whilst you may be prompted to enter your platform 2FA frequently (e.g. when working from home), but you will not need to enter your GitHub 2FA because this is remembered for a month. However, if you have not logged into the platform for more than a month, you will first have to login to GitHub (and enter your GitHub 2FA code), and you will then also be prompted to enter your platform 2FA code. 13.6.1 I’ve lost my platform 2FA If you’ve lost your platform 2FA code because e.g. you’ve broken or lost your phone, please contact the Analytical Platform team and we will reset it for you. 13.6.2 I have entered my 2FA code, but the platform will not accept it Smartphone based 2FA apps require the phone’s clock (the time) to be up to date. If your phone’s clock is out of sync by more than 30 seconds or so, this can cause the 2FA codes to be out of sync. Most phones syncronise their time with the network provider, so this is not a problem. If your time is out of sync, you need to navigate to your clock settings in your phone, and enable the option to sync the time. See e.g. here 13.7 I’m having problems deploying a Shiny app For help resolving deployment problems, see the advanced deployment section of the docs. "],
["annexes.html", "Part 14 Annexes 14.1 Memory limits 14.2 What are the benefits of Github and why do we recommend it? 14.3 Step by step guide to setup Two Factor Authentication 14.4 Data Minimisation", " Part 14 Annexes 14.1 Memory limits All the software running on the Analytical Platform has its memory controlled. Each running software container has a minimum and maximum amount available. Here’s an explanation of the key terms: ‘memory’ is needed for your app and code, but the limiting factor is usually the data that you ‘load into memory’. Data is in memory if it is assigned to a variable, which includes a data frame. a ‘container’ is for one user’s instance of a tool (e.g. Sandra’s R Studio or Bob’s Jupyter) or app (e.g. the PQ Shiny App or the KPIs Web App). ‘Minimum memory’ is the amount the container is guaranteed. It is reserved for it. ‘Maximum memory’ may be available, but your container is competing for this memory with other containers on the server it happens to be running on. If you try to use more than is available or more than the Maximum then your container will be restarted. e.g. Sandra runs her code in R Studio, which loads 8GB of data into a data frame, and she sees in Grafana that this takes 10GB of memory. This is above the minimum of 5GB, and because the servers aren’t too busy the extra 5GB are available. Later the server that her R Studio container is running on happens to get full and when she tries to do something that needs another 1GB memory (i.e. total 11GB) she finds that R Studio restarts, interrupting her work for a few minutes. She restarts her code, but because her R Studio now happens to be running on a different server that is less busy, she finds she can run her code and use 11GB fine. You can monitor your memory usage using Grafana (there is also a link from the Control Panel). Scroll down to “User RAM usage” and you can click on your name to just show yours. Also useful is to adjust the time period from being the “Last 1 hour”, by clicking on the label at the top-right corner. Current memory limits: Container type Minimum Maximum R Studio 5 GB 20 GB Jupyter 1 GB 12 GB App no limit no limit You can work on a dataset that is bigger than your memory by reading in a bit of the data at a time and writing results back to disk as you go. If you’re working on big data then consider taking advantage of tech like Amazon Athena or Apache Spark, which are available through the Analytical Platform too. Our laptops tend to have only 8GB or 16GB, so it’s an advantage of the AP that we can offer more. We are open to increasing the maximum memory, so let us know if you need more, to help us make a case for covering the additional cost. 14.2 What are the benefits of Github and why do we recommend it? Github is a central place to store our analytical projects - particularly those which are built primarily in code. Github keeps track of who wrote what, when, why they wrote it, why we can trust its correctness, and which version of the code was run to produce an analytical result. This is useful if you’re work on your own, but the benefits are greatest when you’re working in a team. Here is some more details of what Git offers: It provides a single, unambigous master version of a project. No more model_final.r, model_finalv2_final.r. etc. It enables you and collaborators to work on the same project and files concurrently, resolving conflicts if you edit the same parts of the same files, whilst keeping track of who made what edits and when. It enables work in progress to be shared with team members, without compromising the master version. You never get confused between what’s not yet final, and what’s trusted, quality assured code. The work in progress can be seemlessly merged into the master version when it’s ready. It provides a history of all previous versions of the projects, which can be meaningfully tagged, and reverted to (undo points). e.g. we may wish to revert to the exact code that was tagged ‘model run 2015Q1’. It reduces the likelihood of code being lost in messy file systems, such as on DOM1. Files sits within defined project ‘repositories’, with all code accessible from this location. It provides an extremely powerful search function. The ability to search all code written by Ministry of Justice analysts in milliseconds. Or all code written by anyone, for that matter. It enables an easier, more robust, more enjoyable approach to quality assurance. In particular, it offers the potential to continuously quality assure a project as it’s built, rather than QA being an activity that’s only done once the work is complete. For example, all additions to the codebase can be reviewed and accepted by a peer before being integrated into the master version. It includes productivity tools like Github issues (essentially a tagged to-do list), and a trello style workflow (Github projects), with automation. Git stores a huge amount of meta data about why changes were made and by whom. This dramatically reduces the danger of code becoming a ‘black box’. The to-do list is automatically linked to the implementation - e.g. The issue of ‘improve number formatting’ is automatically linked to the specific changes in the code that fixed the number formatting. It makes it much easier to build reusable components, and make parts of our code open source (available to the public). For example, we use R code written by statisticians around the world that’s been put on Github, and we know that people across government have been using some of our R code. We can collaborate easily with other government deparments and anyone else for that matter. It makes it easier to ask for help with your work, particularly with colleagues who are working remotely. You can hyperlink and comment on specific lines of code. You can write rich, searchable documentation - e.g. this user guide is hosted on Github! Finally, we have chosed git specifically because it seems to be by far the most popular version control system - see here and here 14.3 Step by step guide to setup Two Factor Authentication Two factor authentication (2FA) is critical to the security of the platform. We have opted to use smartphone based 2FA apps due to the expense of giving out hardware tokens like the RSA device you use to log into DOM1. Note that there are two layers of two factor authentication (2FA) in action on the platform: Github Account 2FA Your Github account must have 2FA enabled. When you log in to Github, your session will stay active for a month before you need to re-enter your 2FA code. Your Github username identifies you to the platform, and we use this identity to control access to data and other resources once you’ve logged into the platform. You therefore must be logged into Github to use the platform. Analytical Platform 2FA Your Analytical Platform account has a separate 2FA step. You will be prompted to set this up the first time you access the platform. This code must be entered once a day. This security step lets you log into the platform and use it. Usually, when you log into the platform, you will be prompted to enter your platform 2FA, but you will not need to enter your Github 2FA because this is remembered for a month. However, if you have not logged into the platform for more than a month, you will first have to login to Github (and enter your Github 2FA code), and you will then also be prompted to enter your platform 2FA code. 14.3.1 Step by step - logging into the platform for the first time The first time you log into the Analytical Platform, you will be asked to set up 2FA. Your welcome email will direct you to the platform Control Panel. 14.3.1.1 Step 1: Log into Github to identify yourself to the Analytical Platform If you’re already logged into Github, you will not see the ‘Sign in to GitHub to continue to Analytics platform’ screen. 14.3.1.2 Step 2: Set up your Platform 2FA using your smartphone In this step, you set up the second layer of 2FA, your Platform 2FA. Scan the code using your smartphone app, and enter the code that comes up on your smartphone. Note: If you get the error ‘Wrong or expired code’, you need to make sure that your phone’s clock is accurate. See here 14.3.1.3 You’re now done Once you’re entered your platform 2FA code in the interface above, you should now have access to the platform. You will need to enter your platform 2FA code around once a day as you use the platform. 14.4 Data Minimisation In order to reduce cybersecurity/information risks to data and promote/maintain a privacy-centered approach you should consider how data can be minimised or obfuscated before being imported if you can do so while maintaining analytical usefulness and data integrity. 14.4.1 Pseudonymisation Pseudonymisation is “…the processing of personal data in such a manner that the personal data can no longer be attributed to a specific data subject without the use of additional information…” An example of pseudonymisation would be replacing the name of an individual with a unique hashed value. This would preserve the concept of an individual person but would mean their name is not stored in the Analytical Platform when it is not directly needed. This is pseudonymisation as the Analytical Platform with the data that it now holds can no longer identify that person without other data that it likely does not have. 14.4.2 Anonymisation Anonymisation is “…information which does not relate to an identified or identifiable natural person or to personal data rendered anonymous in such a manner that the data subject is not or no longer identifiable [even when joined with other accessible data]…” Under current data protection legislation, true anonymisation is quite hard to achieve but should be considered. An example of anonymisation would be using summary values such as where there are 1000 unique ‘users’ in your database, the Analytical Platform held data only holds the ‘1000’ information, not each individual user record. This is anonymisation as the Analytical Platform with the data that it now holds can no longer identify any person at all, as it will be mathematically impossible to use the number ‘1000’ to identify any individual. "],
["conda-package-management.html", "A Conda Package Management A.1 Unified package management for both R and Python A.2 Installing Packages A.3 Platform limitations A.4 Environment Management", " A Conda Package Management A.1 Unified package management for both R and Python A.1.1 Faster builds CRAN1 mirrors do not provide linux2 compiled binaries for packages. This means a long wait when doing install.packages both in an R Studio session and when running a Docker build for a shiny application. A.1.2 Cross Language Both dbtools and s3tools rely on Python packages through the reticulate R to Python bridge. packrat only handles R dependencies, this means that packrat is not enough to reproducibly and reliably manage all of your application’s dependencies. conda supports managing both Python and R dependencies in a single environment. It can make sure all of these libraries are compatible with each other. A.2 Installing Packages If you need to find a package name you can use the anaconda search to find the name. Run conda install PACKAGENAME in the Terminal tab to install it. For more advanced usage have a look at the conda cheat sheet. A.2.1 R Most CRAN (around 95%) are available through conda, they have the same name as the CRAN package name with an additional r- prefix. This is to avoid clashes with Python packages with the same name. A.2.1.1 Examples: A.2.1.1.1 Install a package install.packages (in R-Console) conda install (in Terminal) install.packages('Rcpp') conda install r-Rcpp A.2.1.1.2 Install a specific version of a package install.packages conda install require(devtools) install_version(&quot;ggplot2&quot;, version = &quot;2.2.1&quot;, repos = &quot;http://cran.us.r-project.org&quot;) conda install r-ggplot2=2.2.1 A.2.2 Python Python packages do not require a prefix and can simply be installed using their name. A.2.2.1 Examples A.2.2.1.1 Install a package In the terminal run: conda install numpy which you can now access in your R session library(reticulate) np &lt;- import(&quot;numpy&quot;) np$arange(15) A.2.3 Operating System Packages Even if you want to continue using packrat to manage your R packages you can use conda to resolve operating system dependencies like libxml2. A.2.3.1 Examples A.2.3.1.1 Installing a package that relies on OS dependency Suppose you want to install the R package bigIntegerAlgos but it fails because it depends on a system level library called gmp. Switch to the terminal and use conda to install it. Then switch back to the R console and try to use install.packages again. A.3 Platform limitations Usually when using Conda, it makes sense to have one environment per project, but because we are using the Open Source version of R Studio, there is only a single Conda environment available. This means having to be careful to make sure packages don’t pollute your environment from another project. The following section explains how to manage your environments. A.4 Environment Management A.4.1 Reset your Environment to default Recommended before starting a new project. Will ensure that no unused dependencies are exported when you export an environment.yml for this project. conda env export -n base| grep -v &quot;^prefix: &quot; &gt; /tmp/base.yml &amp;&amp; conda env update --prune -n rstudio -f /tmp/base.yml &amp;&amp; rm /tmp/base.yml A.4.2 Exporting your Environment This is similar to making a packrat.lock file, it catalogues all of the dependencies installed in your environment so that another user can restore a working environment for your application. Check this environment.yml file into your git repository. conda env export | grep -v &quot;^prefix: &quot; &gt; environment.yml A.4.3 Making your R Studio Environment match an environment.yml When checking out a project that has an environment.yml do this to install any packages required by the project that you don’t have in your working environment. conda env update -f environment.yml --prune The “Comprehensive R Archive Network” (CRAN) is a collection of sites which carry identical material, consisting of the R distribution(s), the contributed packages and binaries.↩ Linux (Debian) is the environment that applications in the Analytical Platform run on.↩ "],
["access-local-ports-in-jupyter.html", "B Access Local Ports in Jupyter B.1 Running Plotly Dash apps B.2 Troubleshooting", " B Access Local Ports in Jupyter If you are developing a web application using Jupyter you will probably want to preview that web app before you deploy it. The Analytical Platform provides a special set of endpoints under /_tunnel_/&lt;portNumber&gt;/ that route to exposed HTTP ports on the localhost. There are a few things you need to keep in mind: &lt;portNumber&gt; can only be one from a limited range. By default, you can only route to ports 8050 and 4040–4050 inclusive. You should make your web app listen on one of those. The /_tunnel_/&lt;portNumber&gt;/ endpoint will only tunnel to services bound to a non–public IP address. By default, many web frameworks bind to host 127.0.0.1. You will need to change this to 0.0.0.0 or the tunnel won’t work. This is to prevent inadvertently exposing webapps to the tunnel. B.1 Running Plotly Dash apps B.1.1 Install dependencies In the terminal, install the Dash dependencies: pip install --user dash==0.39.0 # The core dash backend pip install --user dash-daq==0.1.0 # DAQ components (newly open-sourced!) The demo code in this document is known to work with these versions but you can install any version you like for your own code. If you are planning on turning this into a project, you’ll need to manage your dependencies by adding these to either requirements.txt and pip or environment.yaml and conda. B.1.2 Example code Save the Dash hello world app to a new python file called app.py: import dash from dash.dependencies import Input, Output import dash_core_components as dcc import dash_html_components as html import flask import pandas as pd import time import os server = flask.Flask(&#39;app&#39;) server.secret_key = os.environ.get(&#39;secret_key&#39;, &#39;secret&#39;) df = pd.read_csv(&#39;https://raw.githubusercontent.com/plotly/datasets/master/hello-world-stock.csv&#39;) app = dash.Dash(&#39;app&#39;, server=server, url_base_pathname=&#39;/_tunnel_/8050/&#39;) app.scripts.config.serve_locally = False dcc._js_dist[0][&#39;external_url&#39;] = &#39;https://cdn.plot.ly/plotly-basic-latest.min.js&#39; app.layout = html.Div([ html.H1(&#39;Stock Tickers&#39;), dcc.Dropdown( id=&#39;my-dropdown&#39;, options=[ {&#39;label&#39;: &#39;Tesla&#39;, &#39;value&#39;: &#39;TSLA&#39;}, {&#39;label&#39;: &#39;Apple&#39;, &#39;value&#39;: &#39;AAPL&#39;}, {&#39;label&#39;: &#39;Coke&#39;, &#39;value&#39;: &#39;COKE&#39;} ], value=&#39;TSLA&#39; ), dcc.Graph(id=&#39;my-graph&#39;) ], className=&quot;container&quot;) @app.callback(Output(&#39;my-graph&#39;, &#39;figure&#39;), [Input(&#39;my-dropdown&#39;, &#39;value&#39;)]) def update_graph(selected_dropdown_value): dff = df[df[&#39;Stock&#39;] == selected_dropdown_value] return { &#39;data&#39;: [{ &#39;x&#39;: dff.Date, &#39;y&#39;: dff.Close, &#39;line&#39;: { &#39;width&#39;: 3, &#39;shape&#39;: &#39;spline&#39; } }], &#39;layout&#39;: { &#39;margin&#39;: { &#39;l&#39;: 30, &#39;r&#39;: 20, &#39;b&#39;: 30, &#39;t&#39;: 20 } } } if __name__ == &#39;__main__&#39;: app.run_server(host=&#39;0.0.0.0&#39;) There are two important things to note here, where this code differs from the Plotly Dash example: The Dash class must be instantiated with a url_base_pathname. This should always be /_tunnel_/8050/ e.g. app = dash.Dash('app', server=server, url_base_pathname='/_tunnel_/8050/'). When you run the server, it must be bound to 0.0.0.0, e.g., app.run_server(host='0.0.0.0'). B.1.3 Run server from the terminal yovyan@jupyter-lab-r4vi-jupyter-7cb4bb58cf-6hngf:~$ python3 app.py * Serving Flask app &quot;app&quot; (lazy loading) * Environment: production WARNING: Do not use the development server in a production environment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:8050/ (Press CTRL+C to quit) B.1.4 Access via _tunnel_ URL Copy your jupyter URL and append /_tunnel_/8050/ to access your running Dash app. If your jupyter URL is https://r4vi-jupyter-lab.tools.alpha.mojanalytics.xyz/lab?, then your Dash app will be available at https://r4vi-jupyter-lab.tools.alpha.mojanalytics.xyz/_tunnel_/8050/. B.1.4.1 Who can access this /_tunnel_/ URL? Only you can access the URL and it can not be shared with other members of your team. It is intended for testing while developing an application. B.2 Troubleshooting This feature has been introduced in the v0.6.5 jupyer-lab helm chart. If following this guide doesn’t work for you it is likely that you’re on an older version. Contact us on the #analytical_platform Slack channel, alternatively, contact us by email to request an upgrade. "]
]
